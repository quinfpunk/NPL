{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c990e6e",
   "metadata": {},
   "source": [
    "# Lab 03\n",
    "\n",
    "The project is a continuation of what we started on the second lab. You will train a logistic regression classifier on manually extracted features.\n",
    "\n",
    "# Features (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78399c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05494f84",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We load the dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df562cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6b3fb8c8df452e8729269c5ddc91ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "dataset = load_dataset_builder(\"imdb\")\n",
    "dataset_train = load_dataset(\"imdb\", split='train')\n",
    "dataset_test = load_dataset(\"imdb\", split='test')\n",
    "dataset_unsupervised = load_dataset(\"imdb\", split='unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae50c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_filter = ['\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "                      ',', '.', '/', ':', ';', '<', '=', '>', '?', '@',\n",
    "                      '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "def to_lower_case(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    row['text'] = row['text'].lower()\n",
    "    return row\n",
    "\n",
    "def remove_punctuation(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace punctuation from punctuation_filter list to\n",
    "    spaces in the text field of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    for punctuation in punctuation_filter:\n",
    "        row['text'] = row['text'].replace(punctuation, ' ')\n",
    "    row['text'] = row['text'].replace('!', ' ! ')\n",
    "    return row\n",
    "\n",
    "def preprocessing(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict and replace punctuation\n",
    "    from punctuation_filter list to spaces in the text field\n",
    "    of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    return to_lower_case(remove_punctuation(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd53f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_train = dataset_test.map(preprocessing)\n",
    "preprocess_test = dataset_train.map(preprocessing)\n",
    "preprocess_unsupervised = dataset_unsupervised.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11177b22",
   "metadata": {},
   "source": [
    "# Features (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f516e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a16f00",
   "metadata": {},
   "source": [
    "## Get positive and negative lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b4e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('vader_lexicon.txt', delimiter='\t', names=['token', 'sentiment', 'A', 'B'])\n",
    "data = data.drop(columns=['A', 'B'])\n",
    "positive = data.loc[data.sentiment >= 1]\n",
    "positive = positive[\"token\"].values.tolist()\n",
    "negative = data.loc[data.sentiment <= -1]\n",
    "negative = negative[\"token\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bc4f4",
   "metadata": {},
   "source": [
    "## Vectorize text\n",
    "For every given text, we want to generate a vector with the features seen in class.\n",
    "\n",
    "(6 points) Code the following features:\n",
    "\n",
    "- 1 if \"no\" appears in the document, 0 otherwise.\n",
    "- The count of first and second pronouns in the document.\n",
    "- 1 if \"!\" is in the document, 0 otherwise.\n",
    "- Log(word count in the document).\n",
    "- Number of words in the document which are in the positive lexicon.\n",
    "- Number of words in the document which are in the negative lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = ['I', 'you', 'we'] \n",
    "def Vectorizer(documents: list[str]):\n",
    "    X = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        pronoun_count = 0\n",
    "        for word in words:\n",
    "            if word in pronouns:\n",
    "                pronoun_count += 1\n",
    "            if word in positive:\n",
    "                positive_count += 1\n",
    "            elif word in negative:\n",
    "                negative_count += 1\n",
    "\n",
    "        vec = []\n",
    "        # 1 if \"no\" appears in the document, 0 otherwise.\n",
    "        if \"no\" in words:\n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "            \n",
    "        # The count of first and second pronouns in the document.\n",
    "        vec.append(pronoun_count)\n",
    "        \n",
    "        # 1 if \"!\" is in the document, 0 otherwise.\n",
    "        if \"!\" in doc:\n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "        # Log(word count in the document).\n",
    "        vec.append(math.log10(len(words)))\n",
    "    \n",
    "        # positive and negative lexicon\n",
    "    \n",
    "        vec.append(positive_count)\n",
    "        vec.append(negative_count)\n",
    "\n",
    "        X.append(vec)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89de4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = Vectorizer(preprocess_train['text'])\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a37312",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = torch.tensor(dataset, dtype=torch.float32)\n",
    "# norm result btw -1 and 1\n",
    "#pre_labels = np.array(preprocess_train['label']) * 2 - 1\n",
    "pre_labels = np.array(preprocess_train['label'])\n",
    "labels = torch.tensor(pre_labels, dtype=torch.float32).reshape(-1, 1)\n",
    "print('Train dataset:', all_points.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_points,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print('X_train len:', X_train.shape)\n",
    "print('X_test len:', X_test.shape)\n",
    "print('X_valid len:', X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3fff4",
   "metadata": {},
   "source": [
    "# Logistic regression classifier (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a286cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    \"\"\"A linear regression implementation\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: the dimension of the input features.\n",
    "            nb_classes: the number of classes to predict.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, nb_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: the input tensor.\n",
    "        Returns:\n",
    "            The output of the linear layer.\n",
    "        \"\"\"\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42d4b1",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(6, 1)\n",
    "# Stochastic gradient descent\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "# Keeping an eye on the losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # Setting all gradients to zero.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Sending the whole training set through the model.\n",
    "    predictions = model(X_train)\n",
    "    # Computing the loss.\n",
    "    loss = criterion(predictions, y_train)\n",
    "    train_losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    # Computing the gradients and gradient descent.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # When computing the validation loss, we do not want to update the weights.\n",
    "    # torch.no_grad tells PyTorch to not save the necessary data used for\n",
    "    # gradient descent.\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_valid)\n",
    "        loss = criterion(predictions, y_valid)\n",
    "        test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3502f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the losses\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label=\"Training loss\")\n",
    "plt.plot(np.arange(len(test_losses)), test_losses, label=\"Test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf86e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we need to use a sigmoid on the output now.\n",
    "with torch.no_grad():\n",
    "    p_train = torch.sigmoid(model(X_train))\n",
    "    p_train = np.round(p_train.numpy())\n",
    "    training_accuracy = np.mean(p_train == y_train.numpy())\n",
    "    p_valid = torch.sigmoid(model(X_valid))\n",
    "    p_valid = np.round(p_valid.numpy())\n",
    "    valid_accuracy = np.mean(p_valid == y_valid.numpy())\n",
    "    p_test = torch.sigmoid(model(X_test))\n",
    "    p_test = np.round(p_test.numpy())\n",
    "    test_accuracy = np.mean(p_test == y_test.numpy())\n",
    "print(training_accuracy, valid_accuracy, test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting new samples\n",
    "sample = 'This film was a very good one! I really like it.'\n",
    "pre_sample = preprocessing({'text': sample})\n",
    "print(pre_sample)\n",
    "vec_sample = Vectorizer([pre_sample['text']])\n",
    "print(vec_sample)\n",
    "torch.sigmoid(model(torch.tensor(vec_sample, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
