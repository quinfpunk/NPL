{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a7f04a22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7f04a22",
        "outputId": "25f23a2d-ec8b-480b-c27c-1b57d2ac54b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (1.26.15)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchdata) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchdata) (16.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchdata) (1.3.0)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1 spacy-3.5.3\n",
            "2023-06-03 12:55:38.327327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-03 12:55:39.322576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-03 12:55:40.620128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-03 12:55:40.620620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-03 12:55:40.620814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-06-03 12:55:54.586350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-03 12:55:55.571251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-03 12:55:56.834895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-03 12:55:56.835317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-06-03 12:55:56.835489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy sacrebleu torchdata -U\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ba1de79f",
      "metadata": {
        "id": "ba1de79f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccda430-5e5d-43d0-d9b3-6dad084ffeef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6f691e82",
      "metadata": {
        "id": "6f691e82"
      },
      "outputs": [],
      "source": [
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "25ed4bdb",
      "metadata": {
        "id": "25ed4bdb"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31f6b56e",
      "metadata": {
        "id": "31f6b56e"
      },
      "source": [
        "## Encoding\n",
        "In the positional encoding, why are we using a combination of sinus and cosinus ?\n",
        "\n",
        "    We want to modelize for each word the distance with other words. We want the way of calculating this to be length independant, deterministic, output a unique result for each word and it should generalize easily i.e values should be bounded. The best way to generate for each word this distance is using sinus and cosinus. Indeed they answer criterions previously mentioned. And result of cosine and sinus will correspond to the intuition that farther are words from a given word lower (means closer to 0) will be their distance associated, closer thery are bigger will be the distance (bigger means closer to 1 in that case).\n",
        "    Cosiine and sinus have other practical properties like by modleing it like a mtrix multiplication we find that it can be modeled like a linear trnasformation. Also, distance between words is symetrical. That means that in the previous sentence \"words\" is as far of \"distance\" as symetrical is of \"words\".\n",
        "\n",
        "In the Seq2SeqTransformer class,\n",
        "\n",
        "    What is the parameter nhead for?\n",
        "    \n",
        "        The nhead parameter stands for the number of heads of the multi attention head layer.\n",
        "    \n",
        "    What is the point of the generator?\n",
        "    \n",
        "        \"generator\" is a linear layer apply to forward layers. This means every layers will take the same input.\n",
        "    \n",
        "Describe the goal of the create_mask function. Why does it handle differently the source and target masks?\n",
        "    \n",
        "    The create mask function return mask to apply to the output of attentions layer in the decoder. The goal of this masks is to prevent the transformer from making prediction on futur values. It handles source and target masks separatly because we do not necessarly need this precaution for source. Also we could want other behavior and we could have had it if the mask were different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0f02c6f",
      "metadata": {
        "id": "f0f02c6f"
      },
      "outputs": [],
      "source": [
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3d497ed7",
      "metadata": {
        "id": "3d497ed7"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e40b8b22",
      "metadata": {
        "id": "e40b8b22"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(72)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "15ee970a",
      "metadata": {
        "id": "15ee970a"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7c294e4",
      "metadata": {
        "id": "b7c294e4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3_FX1Q0ZqLkE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_FX1Q0ZqLkE",
        "outputId": "9778de24-6c2b-4e0b-dadd-10d3f891c4fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.324, Val loss: 4.110, Epoch time = 45.054s\n",
            "Epoch: 2, Train loss: 3.761, Val loss: 3.339, Epoch time = 44.297s\n",
            "Epoch: 3, Train loss: 3.159, Val loss: 2.901, Epoch time = 43.881s\n",
            "Epoch: 4, Train loss: 2.767, Val loss: 2.640, Epoch time = 44.451s\n",
            "Epoch: 5, Train loss: 2.478, Val loss: 2.460, Epoch time = 43.612s\n",
            "Epoch: 6, Train loss: 2.245, Val loss: 2.326, Epoch time = 44.693s\n",
            "Epoch: 7, Train loss: 2.055, Val loss: 2.212, Epoch time = 43.431s\n",
            "Epoch: 8, Train loss: 1.895, Val loss: 2.147, Epoch time = 44.747s\n",
            "Epoch: 9, Train loss: 1.752, Val loss: 2.083, Epoch time = 43.562s\n",
            "Epoch: 10, Train loss: 1.629, Val loss: 2.024, Epoch time = 44.668s\n",
            "Epoch: 11, Train loss: 1.519, Val loss: 1.986, Epoch time = 43.550s\n",
            "Epoch: 12, Train loss: 1.421, Val loss: 1.995, Epoch time = 44.519s\n",
            "Epoch: 13, Train loss: 1.332, Val loss: 1.996, Epoch time = 43.533s\n",
            "Epoch: 14, Train loss: 1.248, Val loss: 1.961, Epoch time = 44.073s\n",
            "Epoch: 15, Train loss: 1.175, Val loss: 1.933, Epoch time = 43.733s\n",
            "Epoch: 16, Train loss: 1.104, Val loss: 1.920, Epoch time = 43.656s\n",
            "Epoch: 17, Train loss: 1.037, Val loss: 1.936, Epoch time = 44.337s\n",
            "Epoch: 18, Train loss: 0.974, Val loss: 1.941, Epoch time = 44.184s\n",
            "Epoch: 19, Train loss: 0.918, Val loss: 1.957, Epoch time = 44.554s\n",
            "Epoch: 20, Train loss: 0.867, Val loss: 1.958, Epoch time = 43.610s\n",
            "Epoch: 21, Train loss: 0.817, Val loss: 1.947, Epoch time = 44.673s\n",
            "Epoch: 22, Train loss: 0.768, Val loss: 1.961, Epoch time = 43.667s\n",
            "Epoch: 23, Train loss: 0.723, Val loss: 1.970, Epoch time = 44.746s\n",
            "Epoch: 24, Train loss: 0.678, Val loss: 1.978, Epoch time = 43.582s\n",
            "Epoch: 25, Train loss: 0.639, Val loss: 1.988, Epoch time = 44.826s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7a3d2e3f",
      "metadata": {
        "id": "7a3d2e3f"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def top_k_decode(model, src, src_mask, max_len, start_symbol, k, temperature):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # Apply temperature to the probs\n",
        "        logits = prob / temperature\n",
        "\n",
        "        # Apply top-k sampling\n",
        "        filtered_logits, indices = torch.topk(logits, k)\n",
        "        probabilities = nn.Softmax(dim=-1)(filtered_logits)\n",
        "        next_word_index = torch.multinomial(probabilities, num_samples=1).squeeze()\n",
        "        next_word = indices[0][next_word_index.item()]\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def top_p_decode(model, src, src_mask, max_len, start_symbol, p, temperature):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        # Apply temperature to the prob\n",
        "        logits = prob / temperature\n",
        "\n",
        "        # Apply top-p (nucleus) sampling\n",
        "        probabilities = nn.Softmax(dim=-1)(logits)\n",
        "        sorted_logits, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_logits, dim=-1)\n",
        "        mask = cumulative_probs[0] < p\n",
        "        mask[0] = True\n",
        "        top = sorted_logits[0][mask]\n",
        "        next_word_index = torch.multinomial(top, num_samples=1).squeeze()\n",
        "        next_word = sorted_indices[0][next_word_index.item()]\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str, decode_function, **kargs):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = decode_function(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, **kargs).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "143f99f1",
      "metadata": {
        "id": "143f99f1"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_translation(sample: str, target: str = None, k_list: list =[], p_list: list =[], temperature_list: list =[]) -> None:\n",
        "  \"\"\"\n",
        "  Print the result of the different decode functions\n",
        "  sample = the sample to test\n",
        "  target = the target of the sample\n",
        "  k_list = list of the k parameters to test\n",
        "  p_list = list of the p parameters to test\n",
        "  temperature_list = list of the temperature parameters to test\n",
        "  return void\n",
        "  \"\"\"\n",
        "  print(\" ==== \")\n",
        "  print(\"sample: '\" + sample + \"'\")\n",
        "  if target:\n",
        "    print(\"target: '\" + target + \"'\")\n",
        "  \n",
        "  print(\"\\n==== Greedy ====\")\n",
        "  print(\"  '\" + translate(transformer, sample, greedy_decode) + \"'\")\n",
        "  \n",
        "  print(\"\\n==== Top K Decode ====\")\n",
        "  for k in k_list:\n",
        "    for temperature in temperature_list:\n",
        "      pred = translate(transformer, sample, top_k_decode, k=k, temperature=temperature)\n",
        "      print(f\"K={k} Temperature={temperature}: '{pred}'\")\n",
        "  \n",
        "  print(\"\\n==== Top P Decode ====\")\n",
        "\n",
        "  for p in p_list:\n",
        "    for temperature in temperature_list:\n",
        "      pred = translate(transformer, sample, top_p_decode, p=p, temperature=temperature)\n",
        "      print(f\"P={p} Temperature={temperature}: '{pred}'\")"
      ],
      "metadata": {
        "id": "lRWMpPqxh9i7"
      },
      "id": "lRWMpPqxh9i7",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "\n",
        "# Iterate on the i first element of the validation dataset and call test_translation\n",
        "i = 3\n",
        "for src, tgt in val_iter:\n",
        "  test_translation(src, tgt, k_list=[2,3,5,10], p_list=[0.01,0.1,0.15], temperature_list=[1,2,3])\n",
        "  if i < 0:\n",
        "    break\n",
        "  i -= 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAR8RYpUcDqY",
        "outputId": "0431c66e-ee18-445f-df63-97b3012d6cec"
      },
      "id": "yAR8RYpUcDqY",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ==== \n",
            "sample: 'Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen'\n",
            "target: 'A group of men are loading cotton onto a truck'\n",
            "\n",
            "==== Greedy ====\n",
            "  ' A group of men are putting cotton into cotton candy . '\n",
            "\n",
            "==== Top K Decode ====\n",
            "K=2 Temperature=1: ' Group of men loading cotton into a truck . '\n",
            "K=2 Temperature=2: ' A group of guys loading cotton into a truck . '\n",
            "K=2 Temperature=3: ' A group of guys loading cotton candy onto a truck tunnel . '\n",
            "K=3 Temperature=1: ' A group of men are loading into some cotton bowl . '\n",
            "K=3 Temperature=2: ' The group of men is spinning dough into a truck . '\n",
            "K=3 Temperature=3: ' Group of men loading cotton into a truck . '\n",
            "K=5 Temperature=1: ' A group of men are putting cotton into into a truck . '\n",
            "K=5 Temperature=2: ' A group of men are putting device into a truck . '\n",
            "K=5 Temperature=3: ' A group of men making clay bowl raised onto truck . '\n",
            "K=10 Temperature=1: ' A group of men loading cotton into a truck . '\n",
            "K=10 Temperature=2: ' A group of men pole cutting up cotton cotton candy . '\n",
            "K=10 Temperature=3: ' A performance of uniformed men making rubber into some goods . '\n",
            "\n",
            "==== Top P Decode ====\n",
            "P=0.01 Temperature=1: ' A group of men are putting cotton into cotton candy . '\n",
            "P=0.01 Temperature=2: ' A group of men are putting cotton into cotton candy . '\n",
            "P=0.01 Temperature=3: ' A group of men are putting device into a truck . '\n",
            "P=0.1 Temperature=1: ' A group of men are putting cotton into cotton candy . '\n",
            "P=0.1 Temperature=2: ' A group of men are creating into tunnel attached to pull into a truck .'\n",
            "P=0.1 Temperature=3: ' Sports event doing cookies gear Skateboarder ready on train pins . '\n",
            "P=0.15 Temperature=1: ' A group of men are putting cotton into clay device . '\n",
            "P=0.15 Temperature=2: ' A group of men line Skateboarder loading onto a truck . '\n",
            "P=0.15 Temperature=3: ' bunch steps fences desks gets knit fly marker atop snowcapped brick . '\n",
            " ==== \n",
            "sample: 'Ein Mann schläft in einem grünen Raum auf einem Sofa.'\n",
            "target: 'A man sleeping in a green room on a couch.'\n",
            "\n",
            "==== Greedy ====\n",
            "  ' A man sleeping on a couch in a green room . '\n",
            "\n",
            "==== Top K Decode ====\n",
            "K=2 Temperature=1: ' A man sleeping on a couch in a green room . '\n",
            "K=2 Temperature=2: ' A man sleeps on a couch in a green room . '\n",
            "K=2 Temperature=3: ' A man sleeping on a sofa in a green room . '\n",
            "K=3 Temperature=1: ' The man is sleeping on a couch in a green room . '\n",
            "K=3 Temperature=2: ' The man is asleep while in a green room on a couch . '\n",
            "K=3 Temperature=3: ' A male sleeping on the sofa in a green room . '\n",
            "K=5 Temperature=1: ' A man sleeps on a sofa in a green room . '\n",
            "K=5 Temperature=2: ' A man sleeping on a couch in a green room . '\n",
            "K=5 Temperature=3: ' A man sleeping on a sofa inside a green room '\n",
            "K=10 Temperature=1: ' A man sleeping on a couch in a green room . '\n",
            "K=10 Temperature=2: ' A man sleeping on a sofa in a green room . '\n",
            "K=10 Temperature=3: ' In this building one man is sleeping on it . '\n",
            "\n",
            "==== Top P Decode ====\n",
            "P=0.01 Temperature=1: ' A man sleeping on a couch in a green room . '\n",
            "P=0.01 Temperature=2: ' A man sleeping on a couch in a green room . '\n",
            "P=0.01 Temperature=3: ' A man sleeping on a couch in a green room . '\n",
            "P=0.1 Temperature=1: ' A man sleeping on a couch in a green room . '\n",
            "P=0.1 Temperature=2: ' A man sleeping on a couch in a green room . '\n",
            "P=0.1 Temperature=3: ' A man who is sleeping in a green room , placed on an apple phone swing .'\n",
            "P=0.15 Temperature=1: ' A man sleeping on a couch in a green room . '\n",
            "P=0.15 Temperature=2: ' A man sleeping on a couch in a green room . '\n",
            "P=0.15 Temperature=3: ' An parking reads fallen call on a chairs sitting in a green room . '\n",
            " ==== \n",
            "sample: 'Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.'\n",
            "target: 'A boy wearing headphones sits on a woman's shoulders.'\n",
            "\n",
            "==== Greedy ====\n",
            "  ' A boy with headphones sitting on his shoulders sitting on a woman . '\n",
            "\n",
            "==== Top K Decode ====\n",
            "K=2 Temperature=1: ' A boy with earphones in his headphones sitting on a woman 's shoulders . '\n",
            "K=2 Temperature=2: ' A boy wearing headphones sits on a woman shoulders . '\n",
            "K=2 Temperature=3: ' A boy wearing headphones sitting on a woman 's shoulders . '\n",
            "K=3 Temperature=1: ' A young boy wearing headphones sits on his shoulders shoulders . '\n",
            "K=3 Temperature=2: ' A boy wearing headphones sitting on her shoulders on a woman . '\n",
            "K=3 Temperature=3: ' A young boy wearing earphones sits on her shoulders . '\n",
            "K=5 Temperature=1: ' A young boy wearing headphones sits on his shoulders on a woman 's shoulders . '\n",
            "K=5 Temperature=2: ' Boy with earphones on sits with someone shoulders on . '\n",
            "K=5 Temperature=3: ' The boy with a earphones in her headphones on his shoulders . '\n",
            "K=10 Temperature=1: ' A boy with headphones sitting on her shoulders on a lady 's shoulders . '\n",
            "K=10 Temperature=2: ' A young boy wearing earphones sits while on her shoulders . '\n",
            "K=10 Temperature=3: ' This is a boy on her shoulders , and seated another woman is sitting crossed '\n",
            "\n",
            "==== Top P Decode ====\n",
            "P=0.01 Temperature=1: ' A boy with headphones sitting on his shoulders sitting on a woman . '\n",
            "P=0.01 Temperature=2: ' A boy with headphones sitting on his shoulders sitting on a woman . '\n",
            "P=0.01 Temperature=3: ' A boy with headphones sitting on his shoulders on a woman . '\n",
            "P=0.1 Temperature=1: ' A boy with headphones sitting on his shoulders sitting on a woman . '\n",
            "P=0.1 Temperature=2: ' A boy with headphones sitting on his shoulders holds a woman . '\n",
            "P=0.1 Temperature=3: ' Several children with putting herself on paper sitting with 1 female . '\n",
            "P=0.15 Temperature=1: ' A boy with headphones sitting on his shoulders sitting on a woman . '\n",
            "P=0.15 Temperature=2: ' A boy with headphones sitting on a female desk . '\n",
            "P=0.15 Temperature=3: ' A lap smiles bongo concentrates on bench using a brown marker shoulder '\n",
            " ==== \n",
            "sample: 'Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf'\n",
            "target: 'Two men setting up a blue ice fishing hut on an iced over lake'\n",
            "\n",
            "==== Greedy ====\n",
            "  ' Two men building a blue lift on a huge metal lake . '\n",
            "\n",
            "==== Top K Decode ====\n",
            "K=2 Temperature=1: ' Two men are building a blue lift on a huge metal lake . '\n",
            "K=2 Temperature=2: ' Two men are building a blue contraption on a rocky ledge . '\n",
            "K=2 Temperature=3: ' Two men building a blue huge contraption on a rocky lake '\n",
            "K=3 Temperature=1: ' Two men building a blue huge amount of metal boats on a lake . '\n",
            "K=3 Temperature=2: ' 2 men building a blue huge amount of cows on a lake . '\n",
            "K=3 Temperature=3: ' Two men are setting up a clear metal structure in a huge desert setting . '\n",
            "K=5 Temperature=1: ' Two men building a blue mess on a clear lake . '\n",
            "K=5 Temperature=2: ' Two guys building a blue contraption on a rocky structure . '\n",
            "K=5 Temperature=3: ' Two guys are setting up a big contraption on a low platform . '\n",
            "K=10 Temperature=1: ' Two men building a blue lift on a huge metal structure . '\n",
            "K=10 Temperature=2: ' Two men setting up a blue scene on a clear lake . '\n",
            "K=10 Temperature=3: ' Two gentlemen build up a big clear frame on a lush lake building . '\n",
            "\n",
            "==== Top P Decode ====\n",
            "P=0.01 Temperature=1: ' Two men building a blue lift on a huge metal lake . '\n",
            "P=0.01 Temperature=2: ' Two men building a blue lift on a huge dry lake . '\n",
            "P=0.01 Temperature=3: ' Two men building a blue huge interesting landscape on a huge lake . '\n",
            "P=0.1 Temperature=1: ' Two men building a blue desert structure on a huge lake . '\n",
            "P=0.1 Temperature=2: ' Two men building a blue interesting boulder on a heavily canyon . '\n",
            "P=0.1 Temperature=3: ' Two men watch a frame in a clear event . '\n",
            "P=0.15 Temperature=1: ' Two men building a blue contraption on a huge lake . '\n",
            "P=0.15 Temperature=2: ' Two men are pouring a blue huge moving cut on a broken lake . '\n",
            "P=0.15 Temperature=3: ' tiny men standing attempting directions to ride a starting cave made deep lake field . '\n",
            " ==== \n",
            "sample: 'Ein Mann mit beginnender Glatze, der eine rote Rettungsweste trägt, sitzt in einem kleinen Boot.'\n",
            "target: 'A balding man wearing a red life jacket is sitting in a small boat.'\n",
            "\n",
            "==== Greedy ====\n",
            "  ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "\n",
            "==== Top K Decode ====\n",
            "K=2 Temperature=1: ' A balding man wearing a red vest is sitting in a small boat . '\n",
            "K=2 Temperature=2: ' A balding guy wearing a red life jacket sits in a small boat . '\n",
            "K=2 Temperature=3: ' A bald man wearing red a red vest sits in a small boat . '\n",
            "K=3 Temperature=1: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "K=3 Temperature=2: ' A balding man wearing a red life vest sits in a small boat . '\n",
            "K=3 Temperature=3: ' Man with gray life vest sitting inside a small boat . '\n",
            "K=5 Temperature=1: ' A bald man is wearing a red life vest sitting in a small boat . '\n",
            "K=5 Temperature=2: ' One bald man uses red life vest sitting in a small yellow boat . '\n",
            "K=5 Temperature=3: ' A balding male is sitting wearing a red safety jacket while sitting inside a tiny boat . '\n",
            "K=10 Temperature=1: ' A balding man wearing a red life vest sits in a small boat . '\n",
            "K=10 Temperature=2: ' A balding man riding a red raft sitting in a small life jacket . '\n",
            "K=10 Temperature=3: ' A bald man , sitting wearing a red life shirt and siting in the Little boat . '\n",
            "\n",
            "==== Top P Decode ====\n",
            "P=0.01 Temperature=1: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "P=0.01 Temperature=2: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "P=0.01 Temperature=3: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "P=0.1 Temperature=1: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "P=0.1 Temperature=2: ' A curly - guy wearing a red life vest sits in a small boat . '\n",
            "P=0.1 Temperature=3: ' One bald while carving a white guy helmet sit in a small , mirror . '\n",
            "P=0.15 Temperature=1: ' A balding man wearing a red life jacket sits in a small boat . '\n",
            "P=0.15 Temperature=2: ' The age bearded man wearing a red life vest sits in a small boat . '\n",
            "P=0.15 Temperature=3: ' Running aged boy wearing racing using a white Shorts sit in a small clean . '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the BLEU score of the model"
      ],
      "metadata": {
        "id": "imwZVfl9xYxN"
      },
      "id": "imwZVfl9xYxN"
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER"
      ],
      "metadata": {
        "id": "zW66veG2okWq"
      },
      "id": "zW66veG2okWq",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score(k, p, temperature):\n",
        "  \"\"\"\n",
        "  Compute the bleu score on the greedy, top_k and top_p decode.\n",
        "  k = The k parameter for the top_k function\n",
        "  p = The p parameter for the top_p function\n",
        "  temperature = the temperature for the decode functions\n",
        "  return greedy_score, top_k_score, top_p_score\n",
        "  \"\"\"\n",
        "  bleu = BLEU()\n",
        "\n",
        "  val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "  \n",
        "  preds_k = []\n",
        "  preds_p = []\n",
        "  preds_greedy = []\n",
        "  targets = []\n",
        "  for src, tgt in val_iter:\n",
        "    targets.append(tgt)\n",
        "\n",
        "    pred_k = translate(transformer, src, top_k_decode, k=k, temperature=temperature)\n",
        "    preds_k.append(pred_k)\n",
        "\n",
        "    pred_p = translate(transformer, src, top_p_decode, p=p, temperature=temperature)\n",
        "    preds_p.append(pred_p)\n",
        "\n",
        "    pred_greedy = translate(transformer, src, greedy_decode)\n",
        "    preds_greedy.append(pred_greedy)\n",
        "  \n",
        "  targets = [targets]\n",
        "  greedy_score = bleu.corpus_score(preds_greedy, targets)\n",
        "  top_k_score = bleu.corpus_score(preds_k, targets)\n",
        "  top_p_score = bleu.corpus_score(preds_p, targets)\n",
        "\n",
        "  return greedy_score, top_k_score, top_p_score"
      ],
      "metadata": {
        "id": "1DzrpM7RzaIq"
      },
      "id": "1DzrpM7RzaIq",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g, k, p = score(3, 0.15, 3)"
      ],
      "metadata": {
        "id": "k3FRvcBO1okE"
      },
      "id": "k3FRvcBO1okE",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Greedy score: {g}\")\n",
        "print(f\"Top k score: {k}\")\n",
        "print(f\"Top g score: {p}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ln9GPlF1xWk",
        "outputId": "c68fc2b5-4f04-4506-d8c9-b850ff0609e0"
      },
      "id": "7Ln9GPlF1xWk",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy score: BLEU = 35.63 67.9/43.8/28.7/19.1 (BP = 0.997 ratio = 0.997 hyp_len = 13250 ref_len = 13289)\n",
            "Top k score: BLEU = 18.48 53.9/25.5/12.7/6.7 (BP = 1.000 ratio = 1.007 hyp_len = 13376 ref_len = 13289)\n",
            "Top g score: BLEU = 1.39 22.0/3.5/0.7/0.1 (BP = 1.000 ratio = 1.044 hyp_len = 13869 ref_len = 13289)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The first number refers to the final BLEU score.\n",
        "* The next 4 numbers represents the precision value for 1–4 ngram order.\n",
        "* BP is the brevity penalty\n",
        "* ratio indicates the ratio between hypothesis length and reference length\n",
        "* hyp_len refers to the total number of characters for hypothesis text\n",
        "* ref_len is the total number of characters for reference text\n",
        "\n",
        "Here we can see that the greedy decode have the best final BLEU score."
      ],
      "metadata": {
        "id": "ZfwkYx697D7J"
      },
      "id": "ZfwkYx697D7J"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "olqq-QAZkeNC"
      },
      "id": "olqq-QAZkeNC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}