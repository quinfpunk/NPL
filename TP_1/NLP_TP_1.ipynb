{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd21e6b",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "1. The dataset has three splits:\n",
    "    + train\n",
    "    + test\n",
    "    + unsupervised\n",
    "   Splits can be found in the hugging faces page of the dataset.\n",
    "   Or with the function get_dataset_split_names(\"name_of_dataset\")\n",
    "2. Here are the size of datasets:\n",
    "    + Size of the train dataset: 25000\n",
    "    + Size of the test dataset: 25000\n",
    "    + Size of the unsupervised dataset: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_builder(\"imdb\")\n",
    "dataset_train = load_dataset(\"imdb\", split='train')\n",
    "dataset_test = load_dataset(\"imdb\", split='test')\n",
    "dataset_unsupervised = load_dataset(\"imdb\", split='unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ac115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the train dataset: \" + str(len(dataset_train)))\n",
    "print(\"Size of the test dataset: \" + str(len(dataset_test)))\n",
    "print(\"Size of the unsupervised dataset: \" + str(len(dataset_unsupervised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1eabb",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_filter = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "                      ',', '.', '/', ':', ';', '<', '=', '>', '?', '@',\n",
    "                      '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "def to_lower_case(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    row['text'] = row['text'].lower()\n",
    "    return row\n",
    "\n",
    "def remove_punctuation(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace punctuation from punctuation_filter list to\n",
    "    spaces in the text field of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    for punctuation in punctuation_filter:\n",
    "        row['text'] = row['text'].replace(punctuation, ' ')\n",
    "    return row\n",
    "\n",
    "def preprocessing(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict and replace punctuation\n",
    "    from punctuation_filter list to spaces in the text field\n",
    "    of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    return to_lower_case(remove_punctuation(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_train = dataset_test.map(preprocessing)\n",
    "preprocess_test = dataset_train.map(preprocessing)\n",
    "preprocess_unsupervised = dataset_unsupervised.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f7d29",
   "metadata": {},
   "source": [
    "## II. Naive Bayes classifier\n",
    "\n",
    "### Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(documents: Dataset, classes: list):\n",
    "    logprior = {}\n",
    "    loglikelihood = {k: {} for k in classes}\n",
    "    \n",
    "    # Vocabulary of documents\n",
    "    voc = {} # Histogram {word: count}\n",
    "    class_voc = {k: {} for k in classes}\n",
    "    total_count = 0\n",
    "    \n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        words = document['text'].split()\n",
    "        nonlocal total_count\n",
    "        total_count += len(words)\n",
    "        for word in words:\n",
    "            voc.update({word: voc.get(word, 0) + 1})\n",
    "            c = document['label']\n",
    "            class_voc[c].update({word: class_voc[c].get(word, 0) + 1})\n",
    "    \n",
    "    documents.map(update_voc)\n",
    "\n",
    "    # Update total count for loglikelihood formula\n",
    "    total_count += len(voc)\n",
    "\n",
    "    for c in classes:\n",
    "        num_doc = len(documents)\n",
    "        c_docs = documents.filter(lambda doc: doc['label'] == c)\n",
    "        num_c = len(c_docs)\n",
    "        logprior[c] = math.log(num_c / num_doc)\n",
    "\n",
    "        for word in voc.keys():\n",
    "            loglikelihood[c][word] = math.log((class_voc[c].get(word, 0) + 1)/total_count)\n",
    "        \n",
    "    return logprior, loglikelihood, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_str: str, logprior: dict, loglikelihood: dict, classes: list, voc: dict) -> int:\n",
    "    sum_max = None\n",
    "    c_max = None\n",
    "    for c in classes:\n",
    "        sum_c = logprior[c]\n",
    "        for word in test_str.split():\n",
    "            if word in voc:\n",
    "                sum_c += loglikelihood[c][word]\n",
    "        if not sum_max or sum_max < sum_c:\n",
    "            sum_max = sum_c\n",
    "            c_max = c\n",
    "    return c_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1]\n",
    "logprior, loglikelihood, voc = train_naive_bayes(preprocess_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preprocess_test: Dataset, logprior: dict, loglikelihood: dict, classes: list, voc: dict):\n",
    "    confusion = [0, 0, 0, 0] # TP, TN, FP, FN\n",
    "    accuracy=0\n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        nonlocal accuracy\n",
    "        res = test_naive_bayes(document['text'], logprior, loglikelihood, classes, voc)\n",
    "        confusion[1-res + (2 * (1-document['label']))] += 1\n",
    "        accuracy += res == document['label']\n",
    "    preprocess_test.map(update_voc)\n",
    "    accuracy = accuracy/len(preprocess_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(preprocess_test, logprior, loglikelihood, classes, voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc288",
   "metadata": {},
   "source": [
    "### Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf376a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17928539",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda024bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff31611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    do the lemmatization\n",
    "    \"\"\"\n",
    "    lemmas = [token.lemma_ for token in nlp(row['text'])]\n",
    "    \" \".join(lemmas)\n",
    "    row['text'] = lemmas\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization_train = preprocess_train.map(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09756db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980ede1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'and',\n",
       " 'be',\n",
       " 'willing',\n",
       " 'to',\n",
       " 'put',\n",
       " 'up',\n",
       " 'with',\n",
       " 'a',\n",
       " 'lot',\n",
       " ' ',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'movie',\n",
       " 'tv',\n",
       " 'be',\n",
       " 'usually',\n",
       " 'underfunded',\n",
       " ' ',\n",
       " 'under',\n",
       " 'appreciated',\n",
       " 'and',\n",
       " 'misunderstood',\n",
       " ' ',\n",
       " 'I',\n",
       " 'try',\n",
       " 'to',\n",
       " 'like',\n",
       " 'this',\n",
       " ' ',\n",
       " 'I',\n",
       " 'really',\n",
       " 'do',\n",
       " ' ',\n",
       " 'but',\n",
       " 'it',\n",
       " 'be',\n",
       " 'to',\n",
       " 'good',\n",
       " 'tv',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'as',\n",
       " 'babylon',\n",
       " '5',\n",
       " 'be',\n",
       " 'to',\n",
       " 'star',\n",
       " 'trek',\n",
       " ' ',\n",
       " 'the',\n",
       " 'original',\n",
       " '  ',\n",
       " 'silly',\n",
       " 'prosthetic',\n",
       " ' ',\n",
       " 'cheap',\n",
       " 'cardboard',\n",
       " 'set',\n",
       " ' ',\n",
       " 'stilte',\n",
       " 'dialogue',\n",
       " ' ',\n",
       " 'cg',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'match',\n",
       " 'the',\n",
       " 'background',\n",
       " ' ',\n",
       " 'and',\n",
       " 'painfully',\n",
       " 'one',\n",
       " 'dimensional',\n",
       " 'character',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'overcome',\n",
       " 'with',\n",
       " 'a',\n",
       " ' ',\n",
       " 'sci',\n",
       " 'fi',\n",
       " ' ',\n",
       " 'set',\n",
       " '  ',\n",
       " 'I',\n",
       " 'm',\n",
       " 'sure',\n",
       " 'there',\n",
       " 'be',\n",
       " 'those',\n",
       " 'of',\n",
       " 'you',\n",
       " 'out',\n",
       " 'there',\n",
       " 'who',\n",
       " 'think',\n",
       " 'babylon',\n",
       " '5',\n",
       " 'be',\n",
       " 'good',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'tv',\n",
       " ' ',\n",
       " 'it',\n",
       " 's',\n",
       " 'not',\n",
       " ' ',\n",
       " 'it',\n",
       " 's',\n",
       " 'clich√©d',\n",
       " 'and',\n",
       " 'uninspire',\n",
       " '  ',\n",
       " 'while',\n",
       " 'we',\n",
       " 'viewer',\n",
       " 'might',\n",
       " 'like',\n",
       " 'emotion',\n",
       " 'and',\n",
       " 'character',\n",
       " 'development',\n",
       " ' ',\n",
       " 'sci',\n",
       " 'fi',\n",
       " 'be',\n",
       " 'a',\n",
       " 'genre',\n",
       " 'that',\n",
       " 'do',\n",
       " 'not',\n",
       " 'take',\n",
       " 'itself',\n",
       " 'seriously',\n",
       " ' ',\n",
       " 'cf',\n",
       " ' ',\n",
       " 'star',\n",
       " 'trek',\n",
       " '  ',\n",
       " 'it',\n",
       " 'may',\n",
       " 'treat',\n",
       " 'important',\n",
       " 'issue',\n",
       " ' ',\n",
       " 'yet',\n",
       " 'not',\n",
       " 'as',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'philosophy',\n",
       " ' ',\n",
       " 'it',\n",
       " 's',\n",
       " 'really',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'care',\n",
       " 'about',\n",
       " 'the',\n",
       " 'character',\n",
       " 'here',\n",
       " 'as',\n",
       " 'they',\n",
       " 'be',\n",
       " 'not',\n",
       " 'simply',\n",
       " 'foolish',\n",
       " ' ',\n",
       " 'just',\n",
       " 'miss',\n",
       " 'a',\n",
       " 'spark',\n",
       " 'of',\n",
       " 'life',\n",
       " ' ',\n",
       " 'their',\n",
       " 'action',\n",
       " 'and',\n",
       " 'reaction',\n",
       " 'be',\n",
       " 'wooden',\n",
       " 'and',\n",
       " 'predictable',\n",
       " ' ',\n",
       " 'often',\n",
       " 'painful',\n",
       " 'to',\n",
       " 'watch',\n",
       " ' ',\n",
       " 'the',\n",
       " 'maker',\n",
       " 'of',\n",
       " 'earth',\n",
       " 'know',\n",
       " 'it',\n",
       " 's',\n",
       " 'rubbish',\n",
       " 'as',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'always',\n",
       " 'say',\n",
       " ' ',\n",
       " 'gene',\n",
       " 'roddenberry',\n",
       " 's',\n",
       " 'earth',\n",
       " '    ',\n",
       " 'otherwise',\n",
       " 'people',\n",
       " 'would',\n",
       " 'not',\n",
       " 'continue',\n",
       " 'watching',\n",
       " ' ',\n",
       " 'roddenberry',\n",
       " 's',\n",
       " 'ashe',\n",
       " 'must',\n",
       " 'be',\n",
       " 'turn',\n",
       " 'in',\n",
       " 'their',\n",
       " 'orbit',\n",
       " 'as',\n",
       " 'this',\n",
       " 'dull',\n",
       " ' ',\n",
       " 'cheap',\n",
       " ' ',\n",
       " 'poorly',\n",
       " 'edit',\n",
       " ' ',\n",
       " 'watch',\n",
       " 'it',\n",
       " 'without',\n",
       " 'advert',\n",
       " 'break',\n",
       " 'really',\n",
       " 'bring',\n",
       " 'this',\n",
       " 'home',\n",
       " ' ',\n",
       " 'trudge',\n",
       " 'trabant',\n",
       " 'of',\n",
       " 'a',\n",
       " 'show',\n",
       " 'lumber',\n",
       " 'into',\n",
       " 'space',\n",
       " ' ',\n",
       " 'spoiler',\n",
       " ' ',\n",
       " 'so',\n",
       " ' ',\n",
       " 'kill',\n",
       " 'off',\n",
       " 'a',\n",
       " 'main',\n",
       " 'character',\n",
       " ' ',\n",
       " 'and',\n",
       " 'then',\n",
       " 'bring',\n",
       " 'he',\n",
       " 'back',\n",
       " 'as',\n",
       " 'another',\n",
       " 'actor',\n",
       " ' ',\n",
       " 'jeeez',\n",
       " ' ',\n",
       " 'dallas',\n",
       " 'all',\n",
       " 'over',\n",
       " 'again']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization_train[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062d55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
