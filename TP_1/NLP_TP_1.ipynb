{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd21e6b",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "1. The dataset has three splits:\n",
    "    + train\n",
    "    + test\n",
    "    + unsupervised\n",
    "   Splits can be found in the hugging faces page of the dataset.\n",
    "   Or with the function get_dataset_split_names(\"name_of_dataset\")\n",
    "2. Here are the size of datasets:\n",
    "    + Size of the train dataset: 25000\n",
    "    + Size of the test dataset: 25000\n",
    "    + Size of the unsupervised dataset: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_builder(\"imdb\")\n",
    "dataset_train = load_dataset(\"imdb\", split='train')\n",
    "dataset_test = load_dataset(\"imdb\", split='test')\n",
    "dataset_unsupervised = load_dataset(\"imdb\", split='unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ac115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the train dataset: \" + str(len(dataset_train)))\n",
    "print(\"Size of the test dataset: \" + str(len(dataset_test)))\n",
    "print(\"Size of the unsupervised dataset: \" + str(len(dataset_unsupervised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1eabb",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_filter = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "                      ',', '.', '/', ':', ';', '<', '=', '>', '?', '@',\n",
    "                      '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "def to_lower_case(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    row['text'] = row['text'].lower()\n",
    "    return row\n",
    "\n",
    "def remove_punctuation(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace punctuation from punctuation_filter list to\n",
    "    spaces in the text field of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    for punctuation in punctuation_filter:\n",
    "        row['text'] = row['text'].replace(punctuation, ' ')\n",
    "    return row\n",
    "\n",
    "def preprocessing(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict and replace punctuation\n",
    "    from punctuation_filter list to spaces in the text field\n",
    "    of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    return to_lower_case(remove_punctuation(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c3736",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_train = dataset_test.map(preprocessing)\n",
    "preprocess_test = dataset_train.map(preprocessing)\n",
    "preprocess_unsupervised = dataset_unsupervised.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f7d29",
   "metadata": {},
   "source": [
    "## II. Naive Bayes classifier\n",
    "\n",
    "### Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(documents: Dataset, classes: list):\n",
    "    logprior = {}\n",
    "    loglikelihood = {k: {} for k in classes}\n",
    "    \n",
    "    # Vocabulary of documents\n",
    "    voc = {} # Histogram {word: count}\n",
    "    class_voc = {k: {} for k in classes}\n",
    "    total_count = 0\n",
    "    \n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        words = document['text'].split()\n",
    "        nonlocal total_count\n",
    "        total_count += len(words)\n",
    "        for word in words:\n",
    "            voc.update({word: voc.get(word, 0) + 1})\n",
    "            c = document['label']\n",
    "            class_voc[c].update({word: class_voc[c].get(word, 0) + 1})\n",
    "    \n",
    "    documents.map(update_voc)\n",
    "\n",
    "    # Update total count for loglikelihood formula\n",
    "    total_count += len(voc)\n",
    "\n",
    "    for c in classes:\n",
    "        num_doc = len(documents)\n",
    "        c_docs = documents.filter(lambda doc: doc['label'] == c)\n",
    "        num_c = len(c_docs)\n",
    "        logprior[c] = math.log(num_c / num_doc)\n",
    "\n",
    "        for word in voc.keys():\n",
    "            loglikelihood[c][word] = math.log((class_voc[c].get(word, 0) + 1)/total_count)\n",
    "        \n",
    "    return logprior, loglikelihood, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_str: str, logprior: dict, loglikelihood: dict, classes: list, voc: dict) -> int:\n",
    "    sum_max = None\n",
    "    c_max = None\n",
    "    for c in classes:\n",
    "        sum_c = logprior[c]\n",
    "        for word in test_str.split():\n",
    "            if word in voc:\n",
    "                sum_c += loglikelihood[c][word]\n",
    "        if not sum_max or sum_max < sum_c:\n",
    "            sum_max = sum_c\n",
    "            c_max = c\n",
    "    return c_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a830b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0, 1]\n",
    "logprior, loglikelihood, voc = train_naive_bayes(preprocess_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8060d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preprocess_test: Dataset, logprior: dict, loglikelihood: dict, classes: list, voc: dict):\n",
    "    confusion = [0, 0, 0, 0] # TP, TN, FP, FN\n",
    "    accuracy=0\n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        nonlocal accuracy\n",
    "        res = test_naive_bayes(document['text'], logprior, loglikelihood, classes, voc)\n",
    "        confusion[1-res + (2 * (1-document['label']))] += 1\n",
    "        accuracy += res == document['label']\n",
    "    preprocess_test.map(update_voc)\n",
    "    accuracy = accuracy/len(preprocess_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(preprocess_test, logprior, loglikelihood, classes, voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc288",
   "metadata": {},
   "source": [
    "### Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf376a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17928539",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cd7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda024bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a40226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff31611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
