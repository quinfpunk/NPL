{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd21e6b",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "1. The dataset has three splits:\n",
    "    + train\n",
    "    + test\n",
    "    + unsupervised\n",
    "   Splits can be found in the hugging faces page of the dataset.\n",
    "   Or with the function get_dataset_split_names(\"name_of_dataset\")\n",
    "2. Here are the size of datasets:\n",
    "    + Size of the train dataset: 25000\n",
    "    + Size of the test dataset: 25000\n",
    "    + Size of the unsupervised dataset: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be57a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset_builder(\"imdb\")\n",
    "dataset_train = load_dataset(\"imdb\", split='train')\n",
    "dataset_test = load_dataset(\"imdb\", split='test')\n",
    "dataset_unsupervised = load_dataset(\"imdb\", split='unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ac115",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of the train dataset: \" + str(len(dataset_train)))\n",
    "print(\"Size of the test dataset: \" + str(len(dataset_test)))\n",
    "print(\"Size of the unsupervised dataset: \" + str(len(dataset_unsupervised)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1eabb",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "## I. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_filter = ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+',\n",
    "                      ',', '.', '/', ':', ';', '<', '=', '>', '?', '@',\n",
    "                      '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "def to_lower_case(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    row['text'] = row['text'].lower()\n",
    "    return row\n",
    "\n",
    "def remove_punctuation(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replace punctuation from punctuation_filter list to\n",
    "    spaces in the text field of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    for punctuation in punctuation_filter:\n",
    "        row['text'] = row['text'].replace(punctuation, ' ')\n",
    "    return row\n",
    "\n",
    "def preprocessing(row: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Lower text field in the row dict and replace punctuation\n",
    "    from punctuation_filter list to spaces in the text field\n",
    "    of row dict\n",
    "    return: updated row\n",
    "    \"\"\"\n",
    "    return to_lower_case(remove_punctuation(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d5c3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/maxenceoden/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-8085cf80541666ee.arrow\n",
      "Loading cached processed dataset at /home/maxenceoden/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d02788d40065b7bf.arrow\n",
      "Loading cached processed dataset at /home/maxenceoden/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-440a117c0d431674.arrow\n"
     ]
    }
   ],
   "source": [
    "preprocess_train = dataset_test.map(preprocessing)\n",
    "preprocess_test = dataset_train.map(preprocessing)\n",
    "preprocess_unsupervised = dataset_unsupervised.map(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f7d29",
   "metadata": {},
   "source": [
    "## II. Naive Bayes classifier\n",
    "\n",
    "### Our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f918bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(documents: Dataset, classes: list):\n",
    "    logprior = {}\n",
    "    loglikelihood = {k: {} for k in classes}\n",
    "    \n",
    "    # Vocabulary of documents\n",
    "    voc = {} # Histogram {word: count}\n",
    "    class_voc = {k: {} for k in classes}\n",
    "    total_count = 0\n",
    "    \n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        words = document['text'].split()\n",
    "        nonlocal total_count\n",
    "        total_count += len(words)\n",
    "        for word in words:\n",
    "            voc.update({word: voc.get(word, 0) + 1})\n",
    "            c = document['label']\n",
    "            class_voc[c].update({word: class_voc[c].get(word, 0) + 1})\n",
    "    \n",
    "    documents.map(update_voc)\n",
    "\n",
    "    # Update total count for loglikelihood formula\n",
    "    total_count += len(voc)\n",
    "\n",
    "    for c in classes:\n",
    "        num_doc = len(documents)\n",
    "        c_docs = documents.filter(lambda doc: doc['label'] == c)\n",
    "        num_c = len(c_docs)\n",
    "        logprior[c] = math.log(num_c / num_doc)\n",
    "\n",
    "        for word in voc.keys():\n",
    "            loglikelihood[c][word] = math.log((class_voc[c].get(word, 0) + 1)/total_count)\n",
    "        \n",
    "    return logprior, loglikelihood, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c315d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_str: str, logprior: dict, loglikelihood: dict, classes: list, voc: dict) -> int:\n",
    "    sum_max = None\n",
    "    c_max = None\n",
    "    for c in classes:\n",
    "        sum_c = logprior[c]\n",
    "        for word in test_str.split():\n",
    "            if word in voc:\n",
    "                sum_c += loglikelihood[c][word]\n",
    "        if not sum_max or sum_max < sum_c:\n",
    "            sum_max = sum_c\n",
    "            c_max = c\n",
    "    return c_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a830b711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/maxenceoden/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-a8cd3e9d43393ef8.arrow\n",
      "Loading cached processed dataset at /home/maxenceoden/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-b4d91ec42a490537.arrow\n"
     ]
    }
   ],
   "source": [
    "classes = [0, 1]\n",
    "logprior, loglikelihood, voc = train_naive_bayes(preprocess_train, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8060d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preprocess_test: Dataset, logprior: dict, loglikelihood: dict, classes: list, voc: dict):\n",
    "    confusion = [0, 0, 0, 0] # TP, TN, FP, FN\n",
    "    accuracy=0\n",
    "    def update_voc(document: Dataset) -> None:\n",
    "        nonlocal accuracy\n",
    "        res = test_naive_bayes(document['text'], logprior, loglikelihood, classes, voc)\n",
    "        confusion[1-res + (2 * (1-document['label']))] += 1\n",
    "        accuracy += res == document['label']\n",
    "    preprocess_test.map(update_voc)\n",
    "    accuracy = accuracy/len(preprocess_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef40bc1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our implementation accuracy:  0.82152\n"
     ]
    }
   ],
   "source": [
    "print('Our implementation accuracy:', accuracy(preprocess_test, logprior, loglikelihood, classes, voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc288",
   "metadata": {},
   "source": [
    "### Scikit\n",
    "#### 3. Implement a naive Bayes classifier using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecf376a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b19bcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-18 {color: black;background-color: white;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(force_alpha=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(force_alpha=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(force_alpha=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                             lowercase=True, \n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             max_features = 5000)\n",
    "db = preprocess_train\n",
    "y = db['label']\n",
    "corpus = db['text']\n",
    "bag_of_words = vectorizer.fit_transform(corpus)\n",
    "print(bag_of_words.shape)\n",
    "X = bag_of_words.toarray()\n",
    "\n",
    "clf = MultinomialNB(force_alpha=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2357d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer.get_feature_names_out()\n",
    "#print ((clf.feature_log_prob_))\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "#print(vocabulary)\n",
    "vectorizer.transform(['love in my ass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fcd79586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(vectorizer.transform(['I love this movie'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8737b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit learn accuracy: 0.83076\n"
     ]
    }
   ],
   "source": [
    "y = preprocess_test['label']\n",
    "X = vectorizer.transform(preprocess_test['text'])\n",
    "score = clf.score(X, y)\n",
    "print('Scikit learn accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896bda9f",
   "metadata": {},
   "source": [
    "#### 4. Report the accuracy on both training and test set, for both your implementation and the scikit-learn one.\n",
    "- Our implementation accuracy: 0.82152\n",
    "- Scikit learn accuracy: 0.83076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17928539",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620cd7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda024bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a40226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff31611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
