{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47cfb1-3d63-42d0-b29b-3c1ebde6b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1992ac-b345-4bbd-a621-538b6a982f58",
   "metadata": {},
   "source": [
    "# Using GloVe for text classification\n",
    "\n",
    "In this pre-filled notebook, we use GloVe embeddings to train a classifier for sentiment analysis. For every review in the IMDB dataset we:\n",
    "1. Tokenize the review into tokens.\n",
    "2. Get the pre-trained GloVe vector for every token in the review (if they are in the voabulary of GloVe).\n",
    "3. Average the vectors over the full review.\n",
    "4. Send the vector through a logistic regression.\n",
    "\n",
    "This time, we will batch the inputs instead of updating the weights once per epoch.\n",
    "\n",
    "Before starting, to make your experiments reproducible, make sure to [force the random seed](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0cb2c-05a4-4c70-b2dc-84bcdfadbb6d",
   "metadata": {},
   "source": [
    "## Using GloVe (1 point)\n",
    "\n",
    "Let's get familier with GloVe embeddings. We download a small version of GloVe trained of 6 billion words, and use vectors of size 300.\n",
    "\n",
    "The [torchtext documentation](https://pytorch.org/text/stable/vocab.html#glove) being quite poor, you can find details on the different pre-trained vectors on the [Stanford page](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2391a1-f23b-4f98-bac4-651081d6c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip:  70%|████████████████████████████████████████████████████████████████████████████████████▍                                    | 602M/862M [01:53<00:49, 5.23MB/s]"
     ]
    }
   ],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "len(glove.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2630885-032e-44ee-9fb7-b7b571216f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.stoi[\"cat\"], glove.itos[5450], glove.vectors[glove.stoi[\"cat\"]].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5ef6f37-1d1e-4b3a-82d4-ad09e975aa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def2d83-d256-4432-ab2a-ab9cf782b062",
   "metadata": {},
   "source": [
    "Notice that punctuations are part of GloVe's vocabulary.\n",
    "\n",
    "To compare two words, we can look at their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fca0941f-9e61-4cf0-8bb8-a9132d50f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat x cats = 0.6815836429595947\n",
      "cat x dog = 0.6816746592521667\n",
      "cat x fridge = 0.09630905091762543\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"cats\", \"dog\", \"fridge\"]\n",
    "for word in words[1:]:\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[words[0]]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    print(f\"{words[0]} x {word} = {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f960f72-be8e-4f35-8b71-7d188ad61a90",
   "metadata": {},
   "source": [
    "**\\[1 point\\] Find the closest word to \"cat\" in the whole vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a504e211-bb70-48ad-b365-e4ab5d0ed25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6816746592521667\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "word = \"cat\"\n",
    "max_sim = 0\n",
    "word_max_sim = \"\"\n",
    "# your code\n",
    "for pair in glove.stoi:\n",
    "    if (pair == word):\n",
    "        continue;\n",
    "    #print(pair)\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[pair]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    if (similarity > max_sim):\n",
    "        word_max_sim = pair\n",
    "        max_sim = similarity\n",
    "print(max_sim)\n",
    "print(word_max_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afe777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2caa4d00-769b-4304-8010-3721bc926e68",
   "metadata": {},
   "source": [
    "## Dataset and split\n",
    "\n",
    "As we keep the test set for final evaluation, we need to split the training set into a training and validation set. We make sure the split is **stratified** by class (same proportion of class in each split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee47710f-8f8f-4e1b-a3e0-390de0a64d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (C:/Users/rmour/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6844e1a6e1994b91a093f35d575dc4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at C:\\Users\\rmour\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-5f37fd0866e4f89f.arrow and C:\\Users\\rmour\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0\\cache-dd5732a0e6ac784c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f188d7-9e67-4b03-bab1-00508db56399",
   "metadata": {},
   "source": [
    "## Prepare the inputs\n",
    "\n",
    "### Text processing pipeline (2 points)\n",
    "\n",
    "For a given entry, we want to\n",
    "1. Tokenize the text.\n",
    "2. Get the vectors for each token.\n",
    "3. Average them.\n",
    "\n",
    "For tokenization, let's use the \"basic_english\" tokenizer from torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a9868b71-5bc5-429c-9cb1-4ddb9c331f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63219cef-b375-40de-b3ad-1401b6079c5a",
   "metadata": {},
   "source": [
    "**\\[2 points\\] Fill the `preprocess_text` function so it returns the mean of the GloVe vectors of all the tokens within a review.**\n",
    "\n",
    "The two following functions can help.\n",
    "* [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
    "* [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ba4642e-fd7c-4dd1-9799-74360787976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    text: str, vocabulary: vocab.Vocab, tokenizer: Callable[[str], List[str]]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Turn a string into the average of the vectors of its tokens.\n",
    "    Args:\n",
    "        text: the input text.\n",
    "        vocabulary: a pre-trained Vocab object.\n",
    "        tokenizer: a tokenizer taking a text as input and returning a list of tokens.\n",
    "    Returns:\n",
    "        The average tensor over the tokens of the whole text.\n",
    "    \"\"\"\n",
    "    # Your code\n",
    "    list_str = tokenizer(text)\n",
    "    glv_vec = vocabulary.get_vecs_by_tokens(list_str)\n",
    "    return torch.mean(glv_vec, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50572a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0289)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"some text:\"\n",
    "list_str = tokenizer(text)\n",
    "glv_vec = glove.get_vecs_by_tokens(list_str)\n",
    "torch.mean(glv_vec) # option of torch.mean to add to have an output that is not a real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f37a9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = partial(vectorize_text, vocabulary=glove, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b6fa0c3-cb31-49d4-9869-fef8f1bb6a2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert text_pipeline(\"some text.\").shape == torch.Size([300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caf280-7436-44b9-a223-c00012a590d2",
   "metadata": {},
   "source": [
    "Now we turn our 3 sets into vectors and labels.\n",
    "\n",
    "Our data are quite small, so we can keep everything in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6a6f4e6e-6bc6-4bdf-831d-02b6f14a56b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb97d4ffc59a4ecf905d20ab0aa40102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8891022b0d4eea884182cfdb9ab309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc241369044940ec91995a9249b1e19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = [text_pipeline(text) for text in tqdm(train_df[\"text\"])]\n",
    "y_train = train_df[\"label\"]\n",
    "X_valid = [text_pipeline(text) for text in tqdm(valid_df[\"text\"])]\n",
    "y_valid = valid_df[\"label\"]\n",
    "X_test = [text_pipeline(text) for text in tqdm(test_df[\"text\"])]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a5323-3f1e-4d1e-a5fa-2d752c5ae929",
   "metadata": {},
   "source": [
    "### Batch processing (1 point)\n",
    "\n",
    "Instead of doing one update per epoch, we feed the model batches of texts between each update. To do so, we use a simple data generator.\n",
    "\n",
    "**\\[1 point\\] Fill the generator function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "014d3de9-94bb-4be8-9419-8d2aa59ba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    X: List[torch.tensor], y: List[int], batch_size: int = 32\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Yield batches from given input data and labels.\n",
    "    Args:\n",
    "        X: a list of tensor (input features).\n",
    "        y: the corresponding labels.\n",
    "        batch_size: the size of every batch [32].\n",
    "    Returns:\n",
    "        A tuple of tensors (features, labels).\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y)\n",
    "    # Your code \n",
    "    a = torch.stack(X[:batch_size])\n",
    "    b = torch.tensor(y[:batch_size])\n",
    "    # yield the the returning values\n",
    "    \n",
    "    yield a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec4bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04ab809e-7363-4996-a064-291cd97b276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train, batch_size=32)\n",
    "for X, y in train_gen():\n",
    "    assert X.shape == torch.Size([32, 300])\n",
    "    assert y.shape == torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec152a5b-e80e-4d34-82dc-75012707a160",
   "metadata": {},
   "source": [
    "## The classifier (1 point)\n",
    "\n",
    "We create a very simple classifier corresponding a logistic regression.\n",
    "\n",
    "**\\[1 point\\] Fill the classifier's code. The forward function needs to return a logit and not the output of a sigmoid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09607378-c4a4-4e6b-8bf0-b1c40ef941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: the dimension of the input embeddings.\n",
    "        nb_classes: the output dimension.\n",
    "        \"\"\"\n",
    "        # your code\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embedding_size, nb_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: an input tensor\n",
    "        Returns:\n",
    "            Logits.\n",
    "        \"\"\"\n",
    "        # your code\n",
    "         \n",
    "        return self.linear(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c601-8ad2-4244-aa49-380df76091c1",
   "metadata": {},
   "source": [
    "## Training (3 points)\n",
    "\n",
    "We put everything above together and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0e6a6da3-d538-4ebc-9bc9-cdec010de29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47d80e77-75dc-4aa6-b0cf-0f4619bb8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "valid_gen = lambda: data_generator(X_valid, y_valid)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddcbc0-a8a0-433f-86d6-63d51f1877c6",
   "metadata": {},
   "source": [
    "**\\[3 points\\] Fill the following cells. Make sure you save the best model evaluated on the validation set.**\n",
    "* The `deepcopy` function might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe4e2bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1162,  0.0451, -0.0413,  ..., -0.1344, -0.0502,  0.1484],\n",
       "        [-0.1158,  0.0409, -0.0045,  ..., -0.1467, -0.1048,  0.0954],\n",
       "        [-0.1119,  0.0252, -0.0082,  ..., -0.1565, -0.0631,  0.0993],\n",
       "        ...,\n",
       "        [-0.1312,  0.0859, -0.0331,  ..., -0.2094, -0.0564,  0.1217],\n",
       "        [-0.1301,  0.0355, -0.0282,  ..., -0.2001, -0.0048,  0.1127],\n",
       "        [-0.0842,  0.0490,  0.0193,  ..., -0.0957, -0.0885,  0.0884]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_gen())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4ea0bd2-d156-48e4-ab10-82dae2a880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifer(300, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# You can use another optimizer if you want.\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "262b4d32-9d63-4764-92e1-fa7ee27891f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d154decb6b478884783ab39960a465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(a[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Computing the loss.\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\Users\\Administrateur\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Users\\Administrateur\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users\\Administrateur\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3165\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m-> 3165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Long"
     ]
    }
   ],
   "source": [
    "nb_epochs = 50\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "best_model = model\n",
    "best_validation_loss = np.Inf\n",
    "\n",
    "for epoch in tqdm(range(nb_epochs)):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    a = next(train_gen())\n",
    "    predictions = model(a[0])\n",
    "    # Computing the loss.\n",
    "    loss = criterion(predictions, a[1])\n",
    "    train_losses.append(loss.item())\n",
    "    if epoch % 100 == 0:\n",
    "        print(loss)\n",
    "    # Computing the gradients and gradient descent.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # training loop\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    # validation loop\n",
    "    with torch.no_grad():\n",
    "        b = next(valid_gen())\n",
    "        predictions = model(b[0])\n",
    "        loss = criterion(predictions, b[1])\n",
    "        valid_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7907de12-1fc2-4597-980b-87d79baeb807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10c92a46550>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(valid_losses, label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020533b-0522-42ff-bcdc-6d1cd991d332",
   "metadata": {},
   "source": [
    "## Evaluation (3 point)\n",
    "\n",
    "**\\[1 point\\] Compute the accuracy for the 3 splits (training, validation, test).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73674e1e-0be6-49b0-af23-a0a9b518d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b7102-fb1d-4a27-a400-ff0e40e1a685",
   "metadata": {},
   "source": [
    "**\\[1 point\\] For two wrongly classified samples, try guessing why the model was wrong.**\n",
    "\n",
    "**\\[1 point\\] Code a `predict` function which take some text as input and returns a prediction class and score (the output of the sigmoid).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f7ac3-ad53-43e3-a1f6-90ed3a61259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    text: str,\n",
    "    text_pipeline: Callable[[str], torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    device: str,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Return the predicted class and score for a given input.\n",
    "    Args:\n",
    "        text: a given review.\n",
    "        text_pipeline: a function taking a text as input and returning a tensor (model's input).\n",
    "        model: a pre-trained model.\n",
    "        device: the device on which the computation occurs.\n",
    "    Returns:\n",
    "        A tuple (label, score).\n",
    "    \"\"\"\n",
    "    # Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7130f-ce7f-41ed-a7d2-bcc6f65482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In my long years as a movie reviewers, I have seen good and bad movies. But nothing as controversially in the middle.\"\n",
    "predict(text, text_pipeline, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f2dff-da04-466e-bf2c-32f9e84d10a3",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Modify the classifier. Instead of using a simple logistic regression, create a multilayer perceptron. Something like `input -> linear(embedding_size, 128) -> activation function -> linear(128, nb_classes) -> output`, for a two layer perceptron.\n",
    "\n",
    "For the activation function, you can use [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) or [another non-linear activation function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) of your choice.\n",
    "\n",
    "Train your new classifier, look at the loss, and compare its accuracy with the logistic regression. Keep the model with the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975c53d-2707-4d3c-ab8e-6962d5256136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
