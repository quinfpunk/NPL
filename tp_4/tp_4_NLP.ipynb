{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47cfb1-3d63-42d0-b29b-3c1ebde6b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1992ac-b345-4bbd-a621-538b6a982f58",
   "metadata": {},
   "source": [
    "# Using GloVe for text classification\n",
    "\n",
    "In this pre-filled notebook, we use GloVe embeddings to train a classifier for sentiment analysis. For every review in the IMDB dataset we:\n",
    "1. Tokenize the review into tokens.\n",
    "2. Get the pre-trained GloVe vector for every token in the review (if they are in the voabulary of GloVe).\n",
    "3. Average the vectors over the full review.\n",
    "4. Send the vector through a logistic regression.\n",
    "\n",
    "This time, we will batch the inputs instead of updating the weights once per epoch.\n",
    "\n",
    "Before starting, to make your experiments reproducible, make sure to [force the random seed](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0cb2c-05a4-4c70-b2dc-84bcdfadbb6d",
   "metadata": {},
   "source": [
    "## Using GloVe (1 point)\n",
    "\n",
    "Let's get familier with GloVe embeddings. We download a small version of GloVe trained of 6 billion words, and use vectors of size 300.\n",
    "\n",
    "The [torchtext documentation](https://pytorch.org/text/stable/vocab.html#glove) being quite poor, you can find details on the different pre-trained vectors on the [Stanford page](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2391a1-f23b-4f98-bac4-651081d6c258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [02:50, 5.05MB/s]                                                                                                                                                                \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 399999/400000 [01:12<00:00, 5503.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "len(glove.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2630885-032e-44ee-9fb7-b7b571216f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"cat\"], glove.itos[5450], glove.vectors[glove.stoi[\"cat\"]].shape\n",
    "glove.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5ef6f37-1d1e-4b3a-82d4-ad09e975aa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def2d83-d256-4432-ab2a-ab9cf782b062",
   "metadata": {},
   "source": [
    "Notice that punctuations are part of GloVe's vocabulary.\n",
    "\n",
    "To compare two words, we can look at their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca0941f-9e61-4cf0-8bb8-a9132d50f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat x cats = 0.6815836429595947\n",
      "cat x dog = 0.6816746592521667\n",
      "cat x fridge = 0.09630905091762543\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"cats\", \"dog\", \"fridge\"]\n",
    "for word in words[1:]:\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[words[0]]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    print(f\"{words[0]} x {word} = {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f960f72-be8e-4f35-8b71-7d188ad61a90",
   "metadata": {},
   "source": [
    "**\\[1 point\\] Find the closest word to \"cat\" in the whole vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a504e211-bb70-48ad-b365-e4ab5d0ed25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6816746592521667\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "word = \"cat\"\n",
    "max_sim = 0\n",
    "word_max_sim = \"\"\n",
    "# your code\n",
    "for pair in glove.stoi:\n",
    "    if (pair == word):\n",
    "        continue;\n",
    "    #print(pair)\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[pair]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    if (similarity > max_sim):\n",
    "        word_max_sim = pair\n",
    "        max_sim = similarity\n",
    "print(max_sim)\n",
    "print(word_max_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afe777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2caa4d00-769b-4304-8010-3721bc926e68",
   "metadata": {},
   "source": [
    "## Dataset and split\n",
    "\n",
    "As we keep the test set for final evaluation, we need to split the training set into a training and validation set. We make sure the split is **stratified** by class (same proportion of class in each split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee47710f-8f8f-4e1b-a3e0-390de0a64d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e86a7092342488d8b48c19a3fffdaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f188d7-9e67-4b03-bab1-00508db56399",
   "metadata": {},
   "source": [
    "## Prepare the inputs\n",
    "\n",
    "### Text processing pipeline (2 points)\n",
    "\n",
    "For a given entry, we want to\n",
    "1. Tokenize the text.\n",
    "2. Get the vectors for each token.\n",
    "3. Average them.\n",
    "\n",
    "For tokenization, let's use the \"basic_english\" tokenizer from torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9868b71-5bc5-429c-9cb1-4ddb9c331f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63219cef-b375-40de-b3ad-1401b6079c5a",
   "metadata": {},
   "source": [
    "**\\[2 points\\] Fill the `preprocess_text` function so it returns the mean of the GloVe vectors of all the tokens within a review.**\n",
    "\n",
    "The two following functions can help.\n",
    "* [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
    "* [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3ba4642e-fd7c-4dd1-9799-74360787976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    text: str, vocabulary: vocab.Vocab, tokenizer: Callable[[str], List[str]]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Turn a string into the average of the vectors of its tokens.\n",
    "    Args:\n",
    "        text: the input text.\n",
    "        vocabulary: a pre-trained Vocab object.\n",
    "        tokenizer: a tokenizer taking a text as input and returning a list of tokens.\n",
    "    Returns:\n",
    "        The average tensor over the tokens of the whole text.\n",
    "    \"\"\"\n",
    "    # Your code\n",
    "    list_str = tokenizer(text)\n",
    "    print(list_str)\n",
    "    print(vocabulary)\n",
    "    glv_vec = vocabulary.get_vecs_by_tokens(list_str)\n",
    "    #print(glv_vec)\n",
    "    \n",
    "    return torch.mean(glv_vec, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50572a41",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m list_str \u001b[38;5;241m=\u001b[39m tokenizer(text)\n\u001b[1;32m      3\u001b[0m glv_vec \u001b[38;5;241m=\u001b[39m glove\u001b[38;5;241m.\u001b[39mget_vecs_by_tokens(list_str)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglv_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mglv_vec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "text =\"some text:\"\n",
    "list_str = tokenizer(text)\n",
    "glv_vec = glove.get_vecs_by_tokens(list_str)\n",
    "torch.mean(glv_vec) # option of torch.mean to add to have an output that is not a real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a9402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2ebd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b6fa0c3-cb31-49d4-9869-fef8f1bb6a2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', 'text', '.']\n",
      "<torchtext.vocab.vectors.GloVe object at 0x7fc9ade64700>\n",
      "tensor([[-2.0969e-01,  1.8073e-01, -1.6401e-01,  1.3859e-02, -1.2066e-01,\n",
      "         -3.2045e-01,  3.0066e-01,  2.9993e-01,  7.9433e-02, -1.7379e+00,\n",
      "         -3.3073e-01,  3.8947e-01, -3.2170e-02,  4.9593e-01,  4.2202e-01,\n",
      "          1.3376e-01, -3.7729e-01,  1.9272e-01,  4.6088e-02,  1.9894e-01,\n",
      "          1.3188e-01,  1.5433e-01,  3.6765e-01,  5.4241e-02, -3.2795e-01,\n",
      "          1.3680e-02, -2.7777e-01,  3.4166e-01,  7.8530e-02,  4.5871e-01,\n",
      "          2.3654e-01,  4.2045e-01, -6.2128e-01, -9.0873e-02, -8.3579e-01,\n",
      "         -3.3465e-01,  1.7828e-01,  4.9772e-01,  1.6395e-01, -4.6531e-01,\n",
      "          2.1805e-02, -4.8171e-01,  1.5414e-01, -7.1629e-02,  2.5237e-01,\n",
      "          1.5213e-02, -8.8422e-03, -3.2659e-02, -5.7228e-01,  1.4588e-01,\n",
      "          1.3302e-01, -3.2644e-01,  1.3202e-01,  2.0259e-01, -1.4579e-02,\n",
      "          3.8435e-01,  2.6736e-02,  3.0849e-01,  4.5976e-01,  3.0015e-01,\n",
      "          2.4224e-01,  1.9518e-01,  4.7310e-01,  3.5857e-02,  1.5013e-02,\n",
      "         -3.9917e-01,  2.4898e-01,  2.8579e-01,  2.2549e-01, -6.0668e-02,\n",
      "         -1.3603e-01, -2.2580e-01,  2.7984e-01, -1.5356e-01, -2.1182e-03,\n",
      "          3.8267e-02, -3.9469e-02,  6.4721e-02, -4.4501e-01, -2.3758e-01,\n",
      "          8.1949e-02, -3.7228e-01,  1.3755e-01,  1.3578e-01,  2.0902e-01,\n",
      "          1.9981e-01,  1.6244e-01,  1.7905e-03,  3.0419e-01,  2.8779e-01,\n",
      "         -1.8187e-01,  5.7256e-01, -3.5821e-01,  1.6796e-01,  3.2985e-01,\n",
      "         -2.0055e-02, -3.6230e-01, -1.1052e-01,  7.2966e-02, -1.8805e-01,\n",
      "          3.3541e-02, -1.4969e-02, -7.5420e-02, -1.7793e-01, -2.3725e-01,\n",
      "          5.4656e-02, -4.9399e-02, -6.3271e-02, -5.7664e-01,  2.9026e-01,\n",
      "         -1.5747e-01, -5.8418e-01, -1.6611e-01, -2.1650e-01,  1.8862e-01,\n",
      "          4.4795e-01, -2.3116e-01,  3.3462e-01, -9.2686e-02, -3.2044e-01,\n",
      "          1.2368e-01, -2.8712e-01, -2.8113e-01,  6.1539e-01, -2.8942e-01,\n",
      "          4.4092e-01, -9.2358e-02,  3.0767e-01,  3.4047e-01, -1.9798e-01,\n",
      "         -3.2236e-01,  3.9490e-03,  2.8768e-01,  2.3395e-01, -1.0320e-01,\n",
      "         -3.2775e-01, -6.0449e-02, -1.7673e-01,  1.6267e-02,  4.3492e-02,\n",
      "         -6.5866e-01,  5.8802e-02, -6.3032e-02,  2.2571e-01, -4.2642e-01,\n",
      "         -1.8945e-01, -3.3177e-02, -1.2013e-01, -1.9918e-02,  1.1232e-01,\n",
      "         -8.1155e-02, -1.9805e-01, -2.0430e-01,  1.4895e-01,  1.0568e-01,\n",
      "          2.4296e-01, -3.2661e-01,  3.5725e-01,  1.7702e-01,  8.1110e-02,\n",
      "          2.9231e-01, -2.1433e-01, -1.4668e-01,  1.4115e-01, -2.5122e-01,\n",
      "          2.4199e-01,  1.4903e-01,  3.2705e-01,  1.0924e-01,  7.7192e-02,\n",
      "          2.2741e-01, -3.6616e-01, -8.5599e-01,  1.7351e-01, -1.3217e-01,\n",
      "         -2.9890e-01, -2.4293e-02,  3.8800e-01, -9.1872e-02,  5.0352e-01,\n",
      "         -1.0115e-03,  1.4467e-02,  1.6312e-01, -2.4313e-01,  9.9115e-02,\n",
      "         -5.7724e-02,  2.0703e-01, -8.2294e-02,  4.9930e-01, -4.0641e-01,\n",
      "         -4.7454e-01, -2.5543e-02,  1.4246e-01, -9.1742e-02,  2.0739e-01,\n",
      "         -3.3067e-01, -3.7402e-01, -3.2789e-02, -2.0275e-01, -8.0877e-02,\n",
      "          5.5674e-01, -1.8270e-01,  6.0812e-02,  1.9221e-01,  2.2320e-02,\n",
      "          2.2168e-01, -2.5173e-01,  2.7011e-01,  1.3874e-01, -2.8264e-01,\n",
      "          2.0432e-01, -8.0672e-02,  3.5388e-01, -9.8586e-02,  8.3369e-02,\n",
      "         -1.5121e-01,  2.8818e-01,  1.7342e-01, -2.2062e-01,  1.3628e-01,\n",
      "          3.5976e-01, -2.3624e-01, -4.4664e-02,  2.1633e-01,  1.3111e-01,\n",
      "          2.4447e-01,  3.3310e-01,  3.9275e-02, -2.2188e-01, -4.4476e-01,\n",
      "         -6.1959e-02,  1.7481e-01, -2.1065e-01, -9.5587e-02,  5.1568e-01,\n",
      "          3.3611e-01,  1.6244e-01,  1.5870e-01, -6.4295e-01, -7.8202e-02,\n",
      "          2.1825e-02,  2.4242e-01,  3.5120e-01,  1.6036e-01, -9.2623e-01,\n",
      "         -1.2729e-01,  3.1367e-01, -5.1599e-02, -2.8050e-01,  2.5210e-01,\n",
      "          3.2277e-02, -4.4744e-01,  3.2787e-03, -1.9394e-01,  2.8086e-01,\n",
      "          2.1703e-01, -9.8486e-02, -5.8086e-02,  9.9323e-02,  3.4244e-02,\n",
      "          2.3886e-01,  2.8057e-01, -1.7233e-02,  1.7791e-01,  2.4745e-01,\n",
      "         -1.3829e-01, -5.0859e-01,  2.5998e-02,  1.6309e-01,  2.8085e-01,\n",
      "          8.5112e-02,  2.2417e-02, -1.2502e-02, -1.1018e-02,  1.8823e-01,\n",
      "          8.3738e-02, -2.6406e+00, -8.6175e-02,  4.3337e-01,  1.0114e-01,\n",
      "         -3.1946e-01,  1.5640e-01,  2.9308e-01,  9.2835e-03,  1.0203e-01,\n",
      "          1.2804e-01,  5.4758e-02,  6.6123e-02,  2.9337e-01, -4.6825e-02,\n",
      "         -1.6482e-01, -4.5920e-01,  3.2512e-01,  2.1522e-01, -2.8282e-02,\n",
      "         -2.1268e-01, -3.8973e-01, -3.3588e-01, -5.9500e-02, -2.8637e-01],\n",
      "        [-8.1660e-01, -2.7569e-01, -3.2951e-01, -1.9751e-01, -1.3119e-01,\n",
      "          5.8965e-01, -4.7443e-01, -2.0593e-01,  3.2954e-01, -1.5815e+00,\n",
      "          2.2524e-01,  1.9357e-01,  2.6813e-01,  1.8815e-01,  2.0841e-01,\n",
      "         -2.0430e-01, -2.5769e-01, -9.5931e-03,  2.0479e-02, -4.1720e-01,\n",
      "         -2.8133e-01, -3.9980e-01, -1.6343e-01,  5.9352e-01, -5.3312e-02,\n",
      "         -7.3362e-02,  3.9872e-01, -5.3993e-02,  6.2894e-01, -4.0527e-02,\n",
      "          2.0639e-01,  3.1568e-01, -6.2093e-01,  7.4989e-01, -7.9408e-01,\n",
      "         -3.0658e-01,  2.5953e-01, -7.4764e-01, -2.5008e-01,  2.8804e-01,\n",
      "         -1.9341e-01, -4.6537e-02, -4.7254e-01,  8.0808e-02,  4.5168e-02,\n",
      "          3.5310e-01, -1.5433e-01,  2.6329e-01, -6.6062e-01, -6.3936e-01,\n",
      "          4.1719e-01,  5.0924e-01,  3.0314e-01, -3.3813e-01, -1.3836e-01,\n",
      "         -5.3940e-02, -2.5473e-01, -4.2587e-02,  1.0552e-01,  1.9062e-01,\n",
      "          4.0879e-01,  1.8214e-01,  6.2152e-01,  6.2670e-02, -4.9173e-02,\n",
      "         -1.7120e-02,  7.0203e-01, -8.6368e-02,  4.8933e-01,  3.3658e-01,\n",
      "         -2.2803e-01, -4.7276e-01, -3.7096e-01, -5.5353e-02,  2.8143e-01,\n",
      "          8.4028e-02,  2.0456e-03,  3.5074e-01, -1.5634e-01,  4.7819e-01,\n",
      "          9.6793e-02, -2.9262e-01, -2.7872e-01, -3.2396e-02, -1.6140e-01,\n",
      "          2.5144e-01, -7.5474e-01,  8.4637e-01,  5.4590e-01,  2.0688e-01,\n",
      "         -6.4990e-01, -1.1063e-01, -8.2825e-01, -9.9826e-02,  3.8526e-01,\n",
      "          1.4295e-01, -4.7436e-01,  1.2568e-01,  1.5446e-01, -8.9972e-01,\n",
      "          4.6947e-02, -1.9629e-01,  2.7758e-01, -2.7221e-01, -5.7626e-01,\n",
      "          2.2704e-02, -3.1396e-02,  2.8309e-01,  1.7192e-01, -5.0727e-01,\n",
      "          2.3476e-01,  5.1319e-01,  3.3524e-02,  5.7824e-01, -3.3669e-01,\n",
      "         -2.9201e-01, -2.2984e-01,  1.1724e-01,  2.3166e-01, -4.1380e-01,\n",
      "         -2.6799e-01,  2.8777e-01,  1.1001e-02, -3.1379e-01, -3.7060e-01,\n",
      "         -1.9966e-01,  1.2397e-01,  4.6681e-01,  6.4263e-02, -2.6515e-01,\n",
      "         -7.8006e-02,  5.6041e-01, -2.4155e-01, -2.9378e-01, -2.3937e-01,\n",
      "          6.5849e-02,  2.3512e-01,  1.3100e-01,  2.9601e-01, -9.4829e-02,\n",
      "          1.1783e-01, -3.0982e-01, -1.6854e-01, -7.5664e-01, -3.3023e-01,\n",
      "         -3.6737e-01,  5.2573e-02,  2.8259e-01,  4.8538e-01, -1.7577e-01,\n",
      "         -5.6613e-02, -9.2970e-02, -4.6266e-01, -1.2324e-01,  2.2385e-01,\n",
      "         -1.1091e-01,  1.6215e-01,  3.8304e-01, -9.8491e-02, -7.1024e-04,\n",
      "         -5.2973e-01,  2.9904e-01,  5.0885e-01, -4.9836e-01, -3.9503e-01,\n",
      "          2.3334e-01, -1.4898e-01,  7.0313e-01, -8.8450e-01, -1.1316e-01,\n",
      "         -4.2627e-01, -4.0883e-01, -6.5128e-01,  1.4732e-01, -1.8168e-01,\n",
      "         -2.7126e-01,  5.1426e-01,  2.2980e-01, -1.3070e-01, -8.7172e-01,\n",
      "          8.2077e-03, -5.2239e-02,  2.3660e-02, -3.0773e-01, -4.9095e-02,\n",
      "         -6.9301e-01, -8.2229e-01,  1.0820e-01,  5.0096e-01, -2.5125e-01,\n",
      "         -1.7115e-01,  3.9114e-01,  4.0307e-01,  4.4392e-02,  5.1866e-01,\n",
      "         -5.7769e-01, -2.9201e-01,  2.2301e-01, -6.0458e-01,  1.3704e-01,\n",
      "         -3.7610e-01, -2.0126e-01, -4.1027e-03,  6.9014e-02,  5.6895e-01,\n",
      "         -1.1324e+00, -6.7414e-01,  1.5519e-01, -5.1225e-01, -1.1089e-01,\n",
      "          5.3191e-02, -5.8321e-01,  7.6475e-02, -1.1771e-01, -3.3121e-01,\n",
      "          3.1899e-01, -7.9882e-02, -1.3678e-01, -2.7658e-02,  5.7666e-01,\n",
      "         -5.2715e-01,  3.3460e-01, -4.1551e-01, -3.1668e-01,  1.7343e-01,\n",
      "         -1.8589e-01,  6.7473e-02,  1.5036e-01,  1.2508e-01,  1.2189e-01,\n",
      "         -6.9339e-01,  2.0925e-01, -1.5887e-01,  6.9364e-02,  5.3750e-01,\n",
      "         -3.9714e-01, -5.2309e-01, -2.5656e-01, -4.2781e-01, -3.2209e-01,\n",
      "         -6.9612e-02,  4.6973e-01,  4.9113e-01,  3.3311e-01, -8.3836e-02,\n",
      "          8.2623e-02, -7.8056e-01, -4.1998e-03, -2.7064e-01,  2.1440e-01,\n",
      "          1.4289e-01, -1.5166e-01, -3.0223e-01, -2.8289e-01, -5.5147e-01,\n",
      "         -1.1002e-01,  4.3316e-01,  3.0647e-01,  2.5492e-01,  1.3151e-01,\n",
      "         -1.3747e-01,  1.1095e-01, -5.8167e-02,  6.5246e-02,  1.2595e-01,\n",
      "          2.5618e-01, -5.0560e-01, -6.3568e-02, -2.8300e-01, -5.7598e-02,\n",
      "         -7.0350e-01, -2.2823e-01,  1.5328e-01,  3.0489e-01, -2.1143e-01,\n",
      "          4.2984e-02, -1.2862e+00, -2.0104e-01,  1.0089e+00,  2.2855e-01,\n",
      "          2.3161e-01, -9.5032e-02, -4.8620e-01, -4.4624e-01,  2.1555e-01,\n",
      "         -4.7358e-02, -6.8808e-03, -1.7273e-01, -3.2040e-01,  5.7034e-01,\n",
      "          4.0851e-01, -7.1599e-02,  8.1033e-02, -4.1078e-01,  6.0274e-01,\n",
      "         -1.6073e-01,  1.9138e-01, -1.2520e-01, -2.8732e-01, -8.5370e-01],\n",
      "        [-1.2559e-01,  1.3630e-02,  1.0306e-01, -1.0123e-01,  9.8128e-02,\n",
      "          1.3627e-01, -1.0721e-01,  2.3697e-01,  3.2870e-01, -1.6785e+00,\n",
      "          2.2393e-01,  1.2409e-01, -8.6708e-02,  3.3010e-01,  3.4375e-01,\n",
      "         -8.7582e-04, -2.9658e-01,  2.4417e-01, -1.1592e-01, -3.5742e-02,\n",
      "         -1.0830e-02,  2.0776e-01,  2.9285e-01, -7.3491e-02, -1.8598e-01,\n",
      "         -2.0090e-01, -9.5366e-02,  6.3732e-03, -1.3620e-01,  9.2028e-02,\n",
      "         -3.9957e-02,  1.9027e-01, -1.0456e-01,  2.7670e-03, -7.1742e-01,\n",
      "         -1.2915e-01, -1.3451e-03,  2.7002e-01, -5.3023e-02,  2.2148e-01,\n",
      "          1.3881e-01, -1.5051e-01, -1.9150e-01,  1.6402e-01,  9.7484e-02,\n",
      "          5.6841e-02,  3.9789e-01,  4.0725e-01,  1.4802e-01,  2.1569e-01,\n",
      "         -1.0671e-01, -1.0232e-01,  2.4810e-02, -2.2100e-01, -1.0720e-02,\n",
      "          1.4234e-01, -2.8242e-01,  1.9254e-01,  8.6720e-02, -3.8970e-01,\n",
      "          1.1321e-01,  1.3779e-03,  6.4009e-03, -1.6206e-01, -8.2153e-02,\n",
      "         -5.5397e-01,  3.6789e-01, -4.0159e-03,  2.0710e-01, -3.7157e-01,\n",
      "          2.5135e-01, -1.9544e-01, -4.7059e-02,  1.7155e-01, -2.4036e-01,\n",
      "         -4.6086e-02,  1.9429e-01, -1.8939e-01, -7.1974e-03,  6.9481e-02,\n",
      "          5.9175e-02, -1.7585e-01,  1.0653e-01,  1.6933e-01, -3.6122e-02,\n",
      "          2.9911e-02, -1.1830e-01,  1.3916e-01, -3.7951e-02,  1.0690e-01,\n",
      "         -2.6069e-01, -1.0307e-01, -1.2272e-01, -1.5032e-01, -4.2409e-02,\n",
      "          1.3354e-02, -2.8510e-01,  1.1248e-02,  1.6073e-01, -1.6384e-01,\n",
      "          2.1233e-01, -1.8476e-01, -9.0874e-04,  6.6687e-02,  1.6918e-01,\n",
      "         -3.5004e-01,  9.9016e-02,  4.6393e-01, -1.9462e-01,  1.0346e-01,\n",
      "         -2.5668e-01, -3.6516e-01, -1.8963e-01, -2.1933e-01,  2.4634e-02,\n",
      "          6.5627e-02, -1.1120e-01, -1.6400e-01,  1.0874e-02, -8.4688e-02,\n",
      "         -1.4923e-01, -7.0223e-02,  2.8887e-02,  8.3497e-02, -1.6193e-02,\n",
      "         -2.4926e-03,  1.7186e-01,  9.8749e-03,  8.0237e-02,  1.4774e-01,\n",
      "          4.3206e-02,  2.7716e-01,  5.7697e-01, -4.1297e-02,  1.2765e-01,\n",
      "         -9.1517e-02,  1.4132e-01,  8.7579e-02,  9.3224e-02,  1.5346e-02,\n",
      "         -1.9856e-01,  1.7277e-02, -1.0708e-01, -1.3059e-02, -3.7227e-01,\n",
      "          7.8568e-02,  1.6677e-01, -1.5359e-01, -3.3294e-01,  3.6986e-02,\n",
      "          1.1697e-01,  3.9781e-02,  3.8464e-02, -1.6247e-01,  4.1280e-01,\n",
      "         -7.7491e-02,  4.5490e-02,  1.1330e-01,  8.2177e-03, -2.5052e-01,\n",
      "          7.0966e-02, -1.1388e-01, -1.1503e-01, -1.1014e-01,  1.0499e-01,\n",
      "          1.5878e-01, -2.7023e-01, -1.1006e-02,  7.6057e-04,  3.3902e-01,\n",
      "          2.5564e-01,  1.6342e-01, -5.6019e-01,  1.3055e-01,  7.6311e-02,\n",
      "         -2.8334e-02,  2.8721e-01, -2.7844e-02, -1.1561e-01,  3.4925e-01,\n",
      "         -1.2420e-01,  2.1405e-01,  2.4116e-01, -3.1343e-02,  1.0913e-01,\n",
      "         -2.4755e-01, -4.5429e-02, -8.2178e-02, -1.8831e-01,  1.8446e-01,\n",
      "         -9.7074e-02,  3.2395e-01,  1.0658e-01, -2.6676e-01, -2.7311e-01,\n",
      "          1.7181e-02,  2.5796e-01, -2.8048e-01,  3.0790e-01, -2.1800e-01,\n",
      "          8.7415e-01, -1.2297e-01,  1.0991e-01, -2.9797e-01,  1.3394e-01,\n",
      "          1.0615e-01, -1.0789e-01, -3.5976e-01, -1.8311e-01, -4.5133e-01,\n",
      "          3.4967e-02, -1.9847e-01,  2.1965e-01,  8.1520e-02,  2.5810e-01,\n",
      "          4.0173e-02,  3.1394e-02,  1.9069e-01,  7.5800e-02, -6.0638e-02,\n",
      "          2.0739e-01,  9.8390e-03, -2.6930e-01,  6.6515e-02, -1.0711e-01,\n",
      "          5.9916e-03,  2.3284e-01, -5.8663e-02,  9.8993e-02, -8.1464e-02,\n",
      "          6.7004e-02, -1.4305e-01,  2.5506e-01, -3.1971e-01, -3.1070e-02,\n",
      "         -9.2451e-02,  2.9440e-01,  2.8947e-01, -5.9804e-02,  2.4286e-01,\n",
      "         -1.6755e-01,  4.2031e-02,  5.1261e-01,  2.4525e-01, -6.5983e-01,\n",
      "          6.2456e-02,  5.2204e-02, -2.5717e-02, -8.0613e-02,  8.0869e-02,\n",
      "          2.2821e-01, -1.0217e-01, -2.0719e-01, -1.2123e-02,  3.4916e-01,\n",
      "          8.6527e-02,  6.6288e-02, -9.9828e-02,  2.5843e-01,  1.1943e-01,\n",
      "         -1.3667e-01, -4.3962e-01,  2.3704e-01,  3.1296e-02,  7.4701e-02,\n",
      "         -2.2387e-01,  7.8162e-03, -1.9016e-01,  4.4444e-02,  2.0191e-01,\n",
      "         -2.0814e-01, -2.8382e-01,  1.0427e-01, -2.1098e-01,  1.8865e-01,\n",
      "          3.1659e-01, -2.0753e+00, -7.1045e-02,  5.2419e-01,  5.6023e-02,\n",
      "         -2.5295e-01, -6.2168e-02, -1.0989e-01, -3.5755e-01, -7.9244e-02,\n",
      "          3.7472e-01, -2.8353e-01,  1.6337e-01,  1.1165e-01, -9.8002e-02,\n",
      "          6.0148e-02, -1.5619e-01, -1.1949e-01,  2.3445e-01,  8.1367e-02,\n",
      "          2.4618e-01, -1.5242e-01, -3.4224e-01, -2.2394e-02,  1.3684e-01]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m shapetext_pipeline \u001b[38;5;241m=\u001b[39m partial(vectorize_text, vocabulary\u001b[38;5;241m=\u001b[39mglove, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m text_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;241m300\u001b[39m])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shapetext_pipeline = partial(vectorize_text, vocabulary=glove, tokenizer=tokenizer)\n",
    "assert text_pipeline(\"some text.\").shape == torch.Size([300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ebbe3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', 'text', '.']\n",
      "<torchtext.vocab.vectors.GloVe object at 0x7fc9ade64700>\n",
      "tensor([[-2.0969e-01,  1.8073e-01, -1.6401e-01,  1.3859e-02, -1.2066e-01,\n",
      "         -3.2045e-01,  3.0066e-01,  2.9993e-01,  7.9433e-02, -1.7379e+00,\n",
      "         -3.3073e-01,  3.8947e-01, -3.2170e-02,  4.9593e-01,  4.2202e-01,\n",
      "          1.3376e-01, -3.7729e-01,  1.9272e-01,  4.6088e-02,  1.9894e-01,\n",
      "          1.3188e-01,  1.5433e-01,  3.6765e-01,  5.4241e-02, -3.2795e-01,\n",
      "          1.3680e-02, -2.7777e-01,  3.4166e-01,  7.8530e-02,  4.5871e-01,\n",
      "          2.3654e-01,  4.2045e-01, -6.2128e-01, -9.0873e-02, -8.3579e-01,\n",
      "         -3.3465e-01,  1.7828e-01,  4.9772e-01,  1.6395e-01, -4.6531e-01,\n",
      "          2.1805e-02, -4.8171e-01,  1.5414e-01, -7.1629e-02,  2.5237e-01,\n",
      "          1.5213e-02, -8.8422e-03, -3.2659e-02, -5.7228e-01,  1.4588e-01,\n",
      "          1.3302e-01, -3.2644e-01,  1.3202e-01,  2.0259e-01, -1.4579e-02,\n",
      "          3.8435e-01,  2.6736e-02,  3.0849e-01,  4.5976e-01,  3.0015e-01,\n",
      "          2.4224e-01,  1.9518e-01,  4.7310e-01,  3.5857e-02,  1.5013e-02,\n",
      "         -3.9917e-01,  2.4898e-01,  2.8579e-01,  2.2549e-01, -6.0668e-02,\n",
      "         -1.3603e-01, -2.2580e-01,  2.7984e-01, -1.5356e-01, -2.1182e-03,\n",
      "          3.8267e-02, -3.9469e-02,  6.4721e-02, -4.4501e-01, -2.3758e-01,\n",
      "          8.1949e-02, -3.7228e-01,  1.3755e-01,  1.3578e-01,  2.0902e-01,\n",
      "          1.9981e-01,  1.6244e-01,  1.7905e-03,  3.0419e-01,  2.8779e-01,\n",
      "         -1.8187e-01,  5.7256e-01, -3.5821e-01,  1.6796e-01,  3.2985e-01,\n",
      "         -2.0055e-02, -3.6230e-01, -1.1052e-01,  7.2966e-02, -1.8805e-01,\n",
      "          3.3541e-02, -1.4969e-02, -7.5420e-02, -1.7793e-01, -2.3725e-01,\n",
      "          5.4656e-02, -4.9399e-02, -6.3271e-02, -5.7664e-01,  2.9026e-01,\n",
      "         -1.5747e-01, -5.8418e-01, -1.6611e-01, -2.1650e-01,  1.8862e-01,\n",
      "          4.4795e-01, -2.3116e-01,  3.3462e-01, -9.2686e-02, -3.2044e-01,\n",
      "          1.2368e-01, -2.8712e-01, -2.8113e-01,  6.1539e-01, -2.8942e-01,\n",
      "          4.4092e-01, -9.2358e-02,  3.0767e-01,  3.4047e-01, -1.9798e-01,\n",
      "         -3.2236e-01,  3.9490e-03,  2.8768e-01,  2.3395e-01, -1.0320e-01,\n",
      "         -3.2775e-01, -6.0449e-02, -1.7673e-01,  1.6267e-02,  4.3492e-02,\n",
      "         -6.5866e-01,  5.8802e-02, -6.3032e-02,  2.2571e-01, -4.2642e-01,\n",
      "         -1.8945e-01, -3.3177e-02, -1.2013e-01, -1.9918e-02,  1.1232e-01,\n",
      "         -8.1155e-02, -1.9805e-01, -2.0430e-01,  1.4895e-01,  1.0568e-01,\n",
      "          2.4296e-01, -3.2661e-01,  3.5725e-01,  1.7702e-01,  8.1110e-02,\n",
      "          2.9231e-01, -2.1433e-01, -1.4668e-01,  1.4115e-01, -2.5122e-01,\n",
      "          2.4199e-01,  1.4903e-01,  3.2705e-01,  1.0924e-01,  7.7192e-02,\n",
      "          2.2741e-01, -3.6616e-01, -8.5599e-01,  1.7351e-01, -1.3217e-01,\n",
      "         -2.9890e-01, -2.4293e-02,  3.8800e-01, -9.1872e-02,  5.0352e-01,\n",
      "         -1.0115e-03,  1.4467e-02,  1.6312e-01, -2.4313e-01,  9.9115e-02,\n",
      "         -5.7724e-02,  2.0703e-01, -8.2294e-02,  4.9930e-01, -4.0641e-01,\n",
      "         -4.7454e-01, -2.5543e-02,  1.4246e-01, -9.1742e-02,  2.0739e-01,\n",
      "         -3.3067e-01, -3.7402e-01, -3.2789e-02, -2.0275e-01, -8.0877e-02,\n",
      "          5.5674e-01, -1.8270e-01,  6.0812e-02,  1.9221e-01,  2.2320e-02,\n",
      "          2.2168e-01, -2.5173e-01,  2.7011e-01,  1.3874e-01, -2.8264e-01,\n",
      "          2.0432e-01, -8.0672e-02,  3.5388e-01, -9.8586e-02,  8.3369e-02,\n",
      "         -1.5121e-01,  2.8818e-01,  1.7342e-01, -2.2062e-01,  1.3628e-01,\n",
      "          3.5976e-01, -2.3624e-01, -4.4664e-02,  2.1633e-01,  1.3111e-01,\n",
      "          2.4447e-01,  3.3310e-01,  3.9275e-02, -2.2188e-01, -4.4476e-01,\n",
      "         -6.1959e-02,  1.7481e-01, -2.1065e-01, -9.5587e-02,  5.1568e-01,\n",
      "          3.3611e-01,  1.6244e-01,  1.5870e-01, -6.4295e-01, -7.8202e-02,\n",
      "          2.1825e-02,  2.4242e-01,  3.5120e-01,  1.6036e-01, -9.2623e-01,\n",
      "         -1.2729e-01,  3.1367e-01, -5.1599e-02, -2.8050e-01,  2.5210e-01,\n",
      "          3.2277e-02, -4.4744e-01,  3.2787e-03, -1.9394e-01,  2.8086e-01,\n",
      "          2.1703e-01, -9.8486e-02, -5.8086e-02,  9.9323e-02,  3.4244e-02,\n",
      "          2.3886e-01,  2.8057e-01, -1.7233e-02,  1.7791e-01,  2.4745e-01,\n",
      "         -1.3829e-01, -5.0859e-01,  2.5998e-02,  1.6309e-01,  2.8085e-01,\n",
      "          8.5112e-02,  2.2417e-02, -1.2502e-02, -1.1018e-02,  1.8823e-01,\n",
      "          8.3738e-02, -2.6406e+00, -8.6175e-02,  4.3337e-01,  1.0114e-01,\n",
      "         -3.1946e-01,  1.5640e-01,  2.9308e-01,  9.2835e-03,  1.0203e-01,\n",
      "          1.2804e-01,  5.4758e-02,  6.6123e-02,  2.9337e-01, -4.6825e-02,\n",
      "         -1.6482e-01, -4.5920e-01,  3.2512e-01,  2.1522e-01, -2.8282e-02,\n",
      "         -2.1268e-01, -3.8973e-01, -3.3588e-01, -5.9500e-02, -2.8637e-01],\n",
      "        [-8.1660e-01, -2.7569e-01, -3.2951e-01, -1.9751e-01, -1.3119e-01,\n",
      "          5.8965e-01, -4.7443e-01, -2.0593e-01,  3.2954e-01, -1.5815e+00,\n",
      "          2.2524e-01,  1.9357e-01,  2.6813e-01,  1.8815e-01,  2.0841e-01,\n",
      "         -2.0430e-01, -2.5769e-01, -9.5931e-03,  2.0479e-02, -4.1720e-01,\n",
      "         -2.8133e-01, -3.9980e-01, -1.6343e-01,  5.9352e-01, -5.3312e-02,\n",
      "         -7.3362e-02,  3.9872e-01, -5.3993e-02,  6.2894e-01, -4.0527e-02,\n",
      "          2.0639e-01,  3.1568e-01, -6.2093e-01,  7.4989e-01, -7.9408e-01,\n",
      "         -3.0658e-01,  2.5953e-01, -7.4764e-01, -2.5008e-01,  2.8804e-01,\n",
      "         -1.9341e-01, -4.6537e-02, -4.7254e-01,  8.0808e-02,  4.5168e-02,\n",
      "          3.5310e-01, -1.5433e-01,  2.6329e-01, -6.6062e-01, -6.3936e-01,\n",
      "          4.1719e-01,  5.0924e-01,  3.0314e-01, -3.3813e-01, -1.3836e-01,\n",
      "         -5.3940e-02, -2.5473e-01, -4.2587e-02,  1.0552e-01,  1.9062e-01,\n",
      "          4.0879e-01,  1.8214e-01,  6.2152e-01,  6.2670e-02, -4.9173e-02,\n",
      "         -1.7120e-02,  7.0203e-01, -8.6368e-02,  4.8933e-01,  3.3658e-01,\n",
      "         -2.2803e-01, -4.7276e-01, -3.7096e-01, -5.5353e-02,  2.8143e-01,\n",
      "          8.4028e-02,  2.0456e-03,  3.5074e-01, -1.5634e-01,  4.7819e-01,\n",
      "          9.6793e-02, -2.9262e-01, -2.7872e-01, -3.2396e-02, -1.6140e-01,\n",
      "          2.5144e-01, -7.5474e-01,  8.4637e-01,  5.4590e-01,  2.0688e-01,\n",
      "         -6.4990e-01, -1.1063e-01, -8.2825e-01, -9.9826e-02,  3.8526e-01,\n",
      "          1.4295e-01, -4.7436e-01,  1.2568e-01,  1.5446e-01, -8.9972e-01,\n",
      "          4.6947e-02, -1.9629e-01,  2.7758e-01, -2.7221e-01, -5.7626e-01,\n",
      "          2.2704e-02, -3.1396e-02,  2.8309e-01,  1.7192e-01, -5.0727e-01,\n",
      "          2.3476e-01,  5.1319e-01,  3.3524e-02,  5.7824e-01, -3.3669e-01,\n",
      "         -2.9201e-01, -2.2984e-01,  1.1724e-01,  2.3166e-01, -4.1380e-01,\n",
      "         -2.6799e-01,  2.8777e-01,  1.1001e-02, -3.1379e-01, -3.7060e-01,\n",
      "         -1.9966e-01,  1.2397e-01,  4.6681e-01,  6.4263e-02, -2.6515e-01,\n",
      "         -7.8006e-02,  5.6041e-01, -2.4155e-01, -2.9378e-01, -2.3937e-01,\n",
      "          6.5849e-02,  2.3512e-01,  1.3100e-01,  2.9601e-01, -9.4829e-02,\n",
      "          1.1783e-01, -3.0982e-01, -1.6854e-01, -7.5664e-01, -3.3023e-01,\n",
      "         -3.6737e-01,  5.2573e-02,  2.8259e-01,  4.8538e-01, -1.7577e-01,\n",
      "         -5.6613e-02, -9.2970e-02, -4.6266e-01, -1.2324e-01,  2.2385e-01,\n",
      "         -1.1091e-01,  1.6215e-01,  3.8304e-01, -9.8491e-02, -7.1024e-04,\n",
      "         -5.2973e-01,  2.9904e-01,  5.0885e-01, -4.9836e-01, -3.9503e-01,\n",
      "          2.3334e-01, -1.4898e-01,  7.0313e-01, -8.8450e-01, -1.1316e-01,\n",
      "         -4.2627e-01, -4.0883e-01, -6.5128e-01,  1.4732e-01, -1.8168e-01,\n",
      "         -2.7126e-01,  5.1426e-01,  2.2980e-01, -1.3070e-01, -8.7172e-01,\n",
      "          8.2077e-03, -5.2239e-02,  2.3660e-02, -3.0773e-01, -4.9095e-02,\n",
      "         -6.9301e-01, -8.2229e-01,  1.0820e-01,  5.0096e-01, -2.5125e-01,\n",
      "         -1.7115e-01,  3.9114e-01,  4.0307e-01,  4.4392e-02,  5.1866e-01,\n",
      "         -5.7769e-01, -2.9201e-01,  2.2301e-01, -6.0458e-01,  1.3704e-01,\n",
      "         -3.7610e-01, -2.0126e-01, -4.1027e-03,  6.9014e-02,  5.6895e-01,\n",
      "         -1.1324e+00, -6.7414e-01,  1.5519e-01, -5.1225e-01, -1.1089e-01,\n",
      "          5.3191e-02, -5.8321e-01,  7.6475e-02, -1.1771e-01, -3.3121e-01,\n",
      "          3.1899e-01, -7.9882e-02, -1.3678e-01, -2.7658e-02,  5.7666e-01,\n",
      "         -5.2715e-01,  3.3460e-01, -4.1551e-01, -3.1668e-01,  1.7343e-01,\n",
      "         -1.8589e-01,  6.7473e-02,  1.5036e-01,  1.2508e-01,  1.2189e-01,\n",
      "         -6.9339e-01,  2.0925e-01, -1.5887e-01,  6.9364e-02,  5.3750e-01,\n",
      "         -3.9714e-01, -5.2309e-01, -2.5656e-01, -4.2781e-01, -3.2209e-01,\n",
      "         -6.9612e-02,  4.6973e-01,  4.9113e-01,  3.3311e-01, -8.3836e-02,\n",
      "          8.2623e-02, -7.8056e-01, -4.1998e-03, -2.7064e-01,  2.1440e-01,\n",
      "          1.4289e-01, -1.5166e-01, -3.0223e-01, -2.8289e-01, -5.5147e-01,\n",
      "         -1.1002e-01,  4.3316e-01,  3.0647e-01,  2.5492e-01,  1.3151e-01,\n",
      "         -1.3747e-01,  1.1095e-01, -5.8167e-02,  6.5246e-02,  1.2595e-01,\n",
      "          2.5618e-01, -5.0560e-01, -6.3568e-02, -2.8300e-01, -5.7598e-02,\n",
      "         -7.0350e-01, -2.2823e-01,  1.5328e-01,  3.0489e-01, -2.1143e-01,\n",
      "          4.2984e-02, -1.2862e+00, -2.0104e-01,  1.0089e+00,  2.2855e-01,\n",
      "          2.3161e-01, -9.5032e-02, -4.8620e-01, -4.4624e-01,  2.1555e-01,\n",
      "         -4.7358e-02, -6.8808e-03, -1.7273e-01, -3.2040e-01,  5.7034e-01,\n",
      "          4.0851e-01, -7.1599e-02,  8.1033e-02, -4.1078e-01,  6.0274e-01,\n",
      "         -1.6073e-01,  1.9138e-01, -1.2520e-01, -2.8732e-01, -8.5370e-01],\n",
      "        [-1.2559e-01,  1.3630e-02,  1.0306e-01, -1.0123e-01,  9.8128e-02,\n",
      "          1.3627e-01, -1.0721e-01,  2.3697e-01,  3.2870e-01, -1.6785e+00,\n",
      "          2.2393e-01,  1.2409e-01, -8.6708e-02,  3.3010e-01,  3.4375e-01,\n",
      "         -8.7582e-04, -2.9658e-01,  2.4417e-01, -1.1592e-01, -3.5742e-02,\n",
      "         -1.0830e-02,  2.0776e-01,  2.9285e-01, -7.3491e-02, -1.8598e-01,\n",
      "         -2.0090e-01, -9.5366e-02,  6.3732e-03, -1.3620e-01,  9.2028e-02,\n",
      "         -3.9957e-02,  1.9027e-01, -1.0456e-01,  2.7670e-03, -7.1742e-01,\n",
      "         -1.2915e-01, -1.3451e-03,  2.7002e-01, -5.3023e-02,  2.2148e-01,\n",
      "          1.3881e-01, -1.5051e-01, -1.9150e-01,  1.6402e-01,  9.7484e-02,\n",
      "          5.6841e-02,  3.9789e-01,  4.0725e-01,  1.4802e-01,  2.1569e-01,\n",
      "         -1.0671e-01, -1.0232e-01,  2.4810e-02, -2.2100e-01, -1.0720e-02,\n",
      "          1.4234e-01, -2.8242e-01,  1.9254e-01,  8.6720e-02, -3.8970e-01,\n",
      "          1.1321e-01,  1.3779e-03,  6.4009e-03, -1.6206e-01, -8.2153e-02,\n",
      "         -5.5397e-01,  3.6789e-01, -4.0159e-03,  2.0710e-01, -3.7157e-01,\n",
      "          2.5135e-01, -1.9544e-01, -4.7059e-02,  1.7155e-01, -2.4036e-01,\n",
      "         -4.6086e-02,  1.9429e-01, -1.8939e-01, -7.1974e-03,  6.9481e-02,\n",
      "          5.9175e-02, -1.7585e-01,  1.0653e-01,  1.6933e-01, -3.6122e-02,\n",
      "          2.9911e-02, -1.1830e-01,  1.3916e-01, -3.7951e-02,  1.0690e-01,\n",
      "         -2.6069e-01, -1.0307e-01, -1.2272e-01, -1.5032e-01, -4.2409e-02,\n",
      "          1.3354e-02, -2.8510e-01,  1.1248e-02,  1.6073e-01, -1.6384e-01,\n",
      "          2.1233e-01, -1.8476e-01, -9.0874e-04,  6.6687e-02,  1.6918e-01,\n",
      "         -3.5004e-01,  9.9016e-02,  4.6393e-01, -1.9462e-01,  1.0346e-01,\n",
      "         -2.5668e-01, -3.6516e-01, -1.8963e-01, -2.1933e-01,  2.4634e-02,\n",
      "          6.5627e-02, -1.1120e-01, -1.6400e-01,  1.0874e-02, -8.4688e-02,\n",
      "         -1.4923e-01, -7.0223e-02,  2.8887e-02,  8.3497e-02, -1.6193e-02,\n",
      "         -2.4926e-03,  1.7186e-01,  9.8749e-03,  8.0237e-02,  1.4774e-01,\n",
      "          4.3206e-02,  2.7716e-01,  5.7697e-01, -4.1297e-02,  1.2765e-01,\n",
      "         -9.1517e-02,  1.4132e-01,  8.7579e-02,  9.3224e-02,  1.5346e-02,\n",
      "         -1.9856e-01,  1.7277e-02, -1.0708e-01, -1.3059e-02, -3.7227e-01,\n",
      "          7.8568e-02,  1.6677e-01, -1.5359e-01, -3.3294e-01,  3.6986e-02,\n",
      "          1.1697e-01,  3.9781e-02,  3.8464e-02, -1.6247e-01,  4.1280e-01,\n",
      "         -7.7491e-02,  4.5490e-02,  1.1330e-01,  8.2177e-03, -2.5052e-01,\n",
      "          7.0966e-02, -1.1388e-01, -1.1503e-01, -1.1014e-01,  1.0499e-01,\n",
      "          1.5878e-01, -2.7023e-01, -1.1006e-02,  7.6057e-04,  3.3902e-01,\n",
      "          2.5564e-01,  1.6342e-01, -5.6019e-01,  1.3055e-01,  7.6311e-02,\n",
      "         -2.8334e-02,  2.8721e-01, -2.7844e-02, -1.1561e-01,  3.4925e-01,\n",
      "         -1.2420e-01,  2.1405e-01,  2.4116e-01, -3.1343e-02,  1.0913e-01,\n",
      "         -2.4755e-01, -4.5429e-02, -8.2178e-02, -1.8831e-01,  1.8446e-01,\n",
      "         -9.7074e-02,  3.2395e-01,  1.0658e-01, -2.6676e-01, -2.7311e-01,\n",
      "          1.7181e-02,  2.5796e-01, -2.8048e-01,  3.0790e-01, -2.1800e-01,\n",
      "          8.7415e-01, -1.2297e-01,  1.0991e-01, -2.9797e-01,  1.3394e-01,\n",
      "          1.0615e-01, -1.0789e-01, -3.5976e-01, -1.8311e-01, -4.5133e-01,\n",
      "          3.4967e-02, -1.9847e-01,  2.1965e-01,  8.1520e-02,  2.5810e-01,\n",
      "          4.0173e-02,  3.1394e-02,  1.9069e-01,  7.5800e-02, -6.0638e-02,\n",
      "          2.0739e-01,  9.8390e-03, -2.6930e-01,  6.6515e-02, -1.0711e-01,\n",
      "          5.9916e-03,  2.3284e-01, -5.8663e-02,  9.8993e-02, -8.1464e-02,\n",
      "          6.7004e-02, -1.4305e-01,  2.5506e-01, -3.1971e-01, -3.1070e-02,\n",
      "         -9.2451e-02,  2.9440e-01,  2.8947e-01, -5.9804e-02,  2.4286e-01,\n",
      "         -1.6755e-01,  4.2031e-02,  5.1261e-01,  2.4525e-01, -6.5983e-01,\n",
      "          6.2456e-02,  5.2204e-02, -2.5717e-02, -8.0613e-02,  8.0869e-02,\n",
      "          2.2821e-01, -1.0217e-01, -2.0719e-01, -1.2123e-02,  3.4916e-01,\n",
      "          8.6527e-02,  6.6288e-02, -9.9828e-02,  2.5843e-01,  1.1943e-01,\n",
      "         -1.3667e-01, -4.3962e-01,  2.3704e-01,  3.1296e-02,  7.4701e-02,\n",
      "         -2.2387e-01,  7.8162e-03, -1.9016e-01,  4.4444e-02,  2.0191e-01,\n",
      "         -2.0814e-01, -2.8382e-01,  1.0427e-01, -2.1098e-01,  1.8865e-01,\n",
      "          3.1659e-01, -2.0753e+00, -7.1045e-02,  5.2419e-01,  5.6023e-02,\n",
      "         -2.5295e-01, -6.2168e-02, -1.0989e-01, -3.5755e-01, -7.9244e-02,\n",
      "          3.7472e-01, -2.8353e-01,  1.6337e-01,  1.1165e-01, -9.8002e-02,\n",
      "          6.0148e-02, -1.5619e-01, -1.1949e-01,  2.3445e-01,  8.1367e-02,\n",
      "          2.4618e-01, -1.5242e-01, -3.4224e-01, -2.2394e-02,  1.3684e-01]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"some text.\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caf280-7436-44b9-a223-c00012a590d2",
   "metadata": {},
   "source": [
    "Now we turn our 3 sets into vectors and labels.\n",
    "\n",
    "Our data are quite small, so we can keep everything in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f4e6e-6bc6-4bdf-831d-02b6f14a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_pipeline(text) for text in tqdm(train_df[\"text\"])]\n",
    "y_train = train_df[\"label\"]\n",
    "X_valid = [text_pipeline(text) for text in tqdm(valid_df[\"text\"])]\n",
    "y_valid = valid_df[\"label\"]\n",
    "X_test = [text_pipeline(text) for text in tqdm(test_df[\"text\"])]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a5323-3f1e-4d1e-a5fa-2d752c5ae929",
   "metadata": {},
   "source": [
    "### Batch processing (1 point)\n",
    "\n",
    "Instead of doing one update per epoch, we feed the model batches of texts between each update. To do so, we use a simple data generator.\n",
    "\n",
    "**\\[1 point\\] Fill the generator function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d3de9-94bb-4be8-9419-8d2aa59ba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    X: List[torch.tensor], y: List[int], batch_size: int = 32\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Yield batches from given input data and labels.\n",
    "    Args:\n",
    "        X: a list of tensor (input features).\n",
    "        y: the corresponding labels.\n",
    "        batch_size: the size of every batch [32].\n",
    "    Returns:\n",
    "        A tuple of tensors (features, labels).\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y)\n",
    "    # Your code\n",
    "    # yield the the returning values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab809e-7363-4996-a064-291cd97b276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train, batch_size=32)\n",
    "for X, y in train_gen():\n",
    "    assert X.shape == torch.Size([32, 300])\n",
    "    assert y.shape == torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec152a5b-e80e-4d34-82dc-75012707a160",
   "metadata": {},
   "source": [
    "## The classifier (1 point)\n",
    "\n",
    "We create a very simple classifier corresponding a logistic regression.\n",
    "\n",
    "**\\[1 point\\] Fill the classifier's code. The forward function needs to return a logit and not the output of a sigmoid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09607378-c4a4-4e6b-8bf0-b1c40ef941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: the dimension of the input embeddings.\n",
    "        nb_classes: the output dimension.\n",
    "        \"\"\"\n",
    "        # your code\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: an input tensor\n",
    "        Returns:\n",
    "            Logits.\n",
    "        \"\"\"\n",
    "        # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c601-8ad2-4244-aa49-380df76091c1",
   "metadata": {},
   "source": [
    "## Training (3 points)\n",
    "\n",
    "We put everything above together and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a6da3-d538-4ebc-9bc9-cdec010de29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d80e77-75dc-4aa6-b0cf-0f4619bb8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "valid_gen = lambda: data_generator(X_valid, y_valid)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddcbc0-a8a0-433f-86d6-63d51f1877c6",
   "metadata": {},
   "source": [
    "**\\[3 points\\] Fill the following cells. Make sure you save the best model evaluated on the validation set.**\n",
    "* The `deepcopy` function might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea0bd2-d156-48e4-ab10-82dae2a880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifer(300, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# You can use another optimizer if you want.\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b4d32-9d63-4764-92e1-fa7ee27891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 50\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "best_model = model\n",
    "best_validation_loss = np.Inf\n",
    "\n",
    "for epoch in tqdm(range(nb_epochs)):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    # training loop\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    # validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907de12-1fc2-4597-980b-87d79baeb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(valid_losses, label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020533b-0522-42ff-bcdc-6d1cd991d332",
   "metadata": {},
   "source": [
    "## Evaluation (3 point)\n",
    "\n",
    "**\\[1 point\\] Compute the accuracy for the 3 splits (training, validation, test).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73674e1e-0be6-49b0-af23-a0a9b518d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b7102-fb1d-4a27-a400-ff0e40e1a685",
   "metadata": {},
   "source": [
    "**\\[1 point\\] For two wrongly classified samples, try guessing why the model was wrong.**\n",
    "\n",
    "**\\[1 point\\] Code a `predict` function which take some text as input and returns a prediction class and score (the output of the sigmoid).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f7ac3-ad53-43e3-a1f6-90ed3a61259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    text: str,\n",
    "    text_pipeline: Callable[[str], torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    device: str,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Return the predicted class and score for a given input.\n",
    "    Args:\n",
    "        text: a given review.\n",
    "        text_pipeline: a function taking a text as input and returning a tensor (model's input).\n",
    "        model: a pre-trained model.\n",
    "        device: the device on which the computation occurs.\n",
    "    Returns:\n",
    "        A tuple (label, score).\n",
    "    \"\"\"\n",
    "    # Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7130f-ce7f-41ed-a7d2-bcc6f65482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In my long years as a movie reviewers, I have seen good and bad movies. But nothing as controversially in the middle.\"\n",
    "predict(text, text_pipeline, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f2dff-da04-466e-bf2c-32f9e84d10a3",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Modify the classifier. Instead of using a simple logistic regression, create a multilayer perceptron. Something like `input -> linear(embedding_size, 128) -> activation function -> linear(128, nb_classes) -> output`, for a two layer perceptron.\n",
    "\n",
    "For the activation function, you can use [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) or [another non-linear activation function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) of your choice.\n",
    "\n",
    "Train your new classifier, look at the loss, and compare its accuracy with the logistic regression. Keep the model with the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975c53d-2707-4d3c-ab8e-6962d5256136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
