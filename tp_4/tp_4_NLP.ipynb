{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a47cfb1-3d63-42d0-b29b-3c1ebde6b93c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_black'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlab_black\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SCIA/NLP/NPL/nlp-env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2369\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2367\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2369\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/Documents/SCIA/NLP/NPL/nlp-env/lib/python3.10/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing module name.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malready loaded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m extension is already loaded. To reload it, use:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/Documents/SCIA/NLP/NPL/nlp-env/lib/python3.10/site-packages/IPython/core/extensions.py:76\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/Documents/SCIA/NLP/NPL/nlp-env/lib/python3.10/site-packages/IPython/core/extensions.py:91\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 91\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     mod \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[module_str]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab_black'"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1992ac-b345-4bbd-a621-538b6a982f58",
   "metadata": {},
   "source": [
    "# Using GloVe for text classification\n",
    "\n",
    "In this pre-filled notebook, we use GloVe embeddings to train a classifier for sentiment analysis. For every review in the IMDB dataset we:\n",
    "1. Tokenize the review into tokens.\n",
    "2. Get the pre-trained GloVe vector for every token in the review (if they are in the voabulary of GloVe).\n",
    "3. Average the vectors over the full review.\n",
    "4. Send the vector through a logistic regression.\n",
    "\n",
    "This time, we will batch the inputs instead of updating the weights once per epoch.\n",
    "\n",
    "Before starting, to make your experiments reproducible, make sure to [force the random seed](https://pytorch.org/docs/stable/notes/randomness.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e402d1-f6f5-4ee5-8cde-813c2a2b4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, Dict, Generator, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchtext import vocab\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0cb2c-05a4-4c70-b2dc-84bcdfadbb6d",
   "metadata": {},
   "source": [
    "## Using GloVe (1 point)\n",
    "\n",
    "Let's get familier with GloVe embeddings. We download a small version of GloVe trained of 6 billion words, and use vectors of size 300.\n",
    "\n",
    "The [torchtext documentation](https://pytorch.org/text/stable/vocab.html#glove) being quite poor, you can find details on the different pre-trained vectors on the [Stanford page](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2391a1-f23b-4f98-bac4-651081d6c258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "len(glove.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2630885-032e-44ee-9fb7-b7b571216f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.stoi[\"cat\"], glove.itos[5450], glove.vectors[glove.stoi[\"cat\"]].shape\n",
    "glove.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ef6f37-1d1e-4b3a-82d4-ad09e975aa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.itos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8def2d83-d256-4432-ab2a-ab9cf782b062",
   "metadata": {},
   "source": [
    "Notice that punctuations are part of GloVe's vocabulary.\n",
    "\n",
    "To compare two words, we can look at their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca0941f-9e61-4cf0-8bb8-a9132d50f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat x cats = 0.6815836429595947\n",
      "cat x dog = 0.6816746592521667\n",
      "cat x fridge = 0.09630905091762543\n"
     ]
    }
   ],
   "source": [
    "words = [\"cat\", \"cats\", \"dog\", \"fridge\"]\n",
    "for word in words[1:]:\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[words[0]]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    print(f\"{words[0]} x {word} = {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f960f72-be8e-4f35-8b71-7d188ad61a90",
   "metadata": {},
   "source": [
    "**\\[1 point\\] Find the closest word to \"cat\" in the whole vocabulary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a504e211-bb70-48ad-b365-e4ab5d0ed25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6816746592521667\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "word = \"cat\"\n",
    "max_sim = 0\n",
    "word_max_sim = \"\"\n",
    "# your code\n",
    "for pair in glove.stoi:\n",
    "    if (pair == word):\n",
    "        continue;\n",
    "    #print(pair)\n",
    "    similarity = torch.cosine_similarity(\n",
    "        glove.vectors[glove.stoi[word]].reshape(1, -1),\n",
    "        glove.vectors[glove.stoi[pair]].reshape(1, -1),\n",
    "    ).item()  # .item() is used to turn a tensor of a single value to a float\n",
    "    if (similarity > max_sim):\n",
    "        word_max_sim = pair\n",
    "        max_sim = similarity\n",
    "print(max_sim)\n",
    "print(word_max_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afe777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2caa4d00-769b-4304-8010-3721bc926e68",
   "metadata": {},
   "source": [
    "## Dataset and split\n",
    "\n",
    "As we keep the test set for final evaluation, we need to split the training set into a training and validation set. We make sure the split is **stratified** by class (same proportion of class in each split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee47710f-8f8f-4e1b-a3e0-390de0a64d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3fc9c50dae41faa5eb575259419ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-5f37fd0866e4f89f.arrow and /home/timothee/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-dd5732a0e6ac784c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20000, 2), (5000, 2), (25000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].train_test_split(\n",
    "    stratify_by_column=\"label\", test_size=0.2, seed=42\n",
    ")\n",
    "test_df = dataset[\"test\"]\n",
    "train_df = train_dataset[\"train\"]\n",
    "valid_df = train_dataset[\"test\"]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f188d7-9e67-4b03-bab1-00508db56399",
   "metadata": {},
   "source": [
    "## Prepare the inputs\n",
    "\n",
    "### Text processing pipeline (2 points)\n",
    "\n",
    "For a given entry, we want to\n",
    "1. Tokenize the text.\n",
    "2. Get the vectors for each token.\n",
    "3. Average them.\n",
    "\n",
    "For tokenization, let's use the \"basic_english\" tokenizer from torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9868b71-5bc5-429c-9cb1-4ddb9c331f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\", language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63219cef-b375-40de-b3ad-1401b6079c5a",
   "metadata": {},
   "source": [
    "**\\[2 points\\] Fill the `preprocess_text` function so it returns the mean of the GloVe vectors of all the tokens within a review.**\n",
    "\n",
    "The two following functions can help.\n",
    "* [torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
    "* [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba4642e-fd7c-4dd1-9799-74360787976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(\n",
    "    text: str, vocabulary: vocab.Vocab, tokenizer: Callable[[str], List[str]]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Turn a string into the average of the vectors of its tokens.\n",
    "    Args:\n",
    "        text: the input text.\n",
    "        vocabulary: a pre-trained Vocab object.\n",
    "        tokenizer: a tokenizer taking a text as input and returning a list of tokens.\n",
    "    Returns:\n",
    "        The average tensor over the tokens of the whole text.\n",
    "    \"\"\"\n",
    "    # Your code\n",
    "    list_str = tokenizer(text)\n",
    "    print(list_str)\n",
    "    print(vocabulary)\n",
    "    glv_vec = vocabulary.get_vecs_by_tokens(list_str)\n",
    "    #print(glv_vec)\n",
    "    \n",
    "    return torch.mean(glv_vec, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50572a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0289)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =\"some text:\"\n",
    "list_str = tokenizer(text)\n",
    "glv_vec = glove.get_vecs_by_tokens(list_str)\n",
    "torch.mean(glv_vec) # option of torch.mean to add to have an output that is not a real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a9402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2ebd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b6fa0c3-cb31-49d4-9869-fef8f1bb6a2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['some', 'text', '.']\n",
      "<torchtext.vocab.vectors.GloVe object at 0x7fc1c820ceb0>\n",
      "torch.Size([3])\n",
      "['some', 'text', '.']\n",
      "<torchtext.vocab.vectors.GloVe object at 0x7fc1c820ceb0>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m text_pipeline \u001b[38;5;241m=\u001b[39m partial(vectorize_text, vocabulary\u001b[38;5;241m=\u001b[39mglove, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m text_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([\u001b[38;5;241m300\u001b[39m])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_pipeline = partial(vectorize_text, vocabulary=glove, tokenizer=tokenizer)\n",
    "print(text_pipeline(\"some text.\").shape)\n",
    "assert text_pipeline(\"some text.\").shape == torch.Size([300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebbe3193",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_pipeline\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msome text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "text_pipeline(\"some text.\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3caf280-7436-44b9-a223-c00012a590d2",
   "metadata": {},
   "source": [
    "Now we turn our 3 sets into vectors and labels.\n",
    "\n",
    "Our data are quite small, so we can keep everything in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f4e6e-6bc6-4bdf-831d-02b6f14a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_pipeline(text) for text in tqdm(train_df[\"text\"])]\n",
    "y_train = train_df[\"label\"]\n",
    "X_valid = [text_pipeline(text) for text in tqdm(valid_df[\"text\"])]\n",
    "y_valid = valid_df[\"label\"]\n",
    "X_test = [text_pipeline(text) for text in tqdm(test_df[\"text\"])]\n",
    "y_test = test_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a5323-3f1e-4d1e-a5fa-2d752c5ae929",
   "metadata": {},
   "source": [
    "### Batch processing (1 point)\n",
    "\n",
    "Instead of doing one update per epoch, we feed the model batches of texts between each update. To do so, we use a simple data generator.\n",
    "\n",
    "**\\[1 point\\] Fill the generator function.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d3de9-94bb-4be8-9419-8d2aa59ba3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(\n",
    "    X: List[torch.tensor], y: List[int], batch_size: int = 32\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \"\"\"\n",
    "    Yield batches from given input data and labels.\n",
    "    Args:\n",
    "        X: a list of tensor (input features).\n",
    "        y: the corresponding labels.\n",
    "        batch_size: the size of every batch [32].\n",
    "    Returns:\n",
    "        A tuple of tensors (features, labels).\n",
    "    \"\"\"\n",
    "    X, y = shuffle(X, y)\n",
    "    # Your code\n",
    "    # yield the the returning values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab809e-7363-4996-a064-291cd97b276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train, batch_size=32)\n",
    "for X, y in train_gen():\n",
    "    assert X.shape == torch.Size([32, 300])\n",
    "    assert y.shape == torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec152a5b-e80e-4d34-82dc-75012707a160",
   "metadata": {},
   "source": [
    "## The classifier (1 point)\n",
    "\n",
    "We create a very simple classifier corresponding a logistic regression.\n",
    "\n",
    "**\\[1 point\\] Fill the classifier's code. The forward function needs to return a logit and not the output of a sigmoid.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09607378-c4a4-4e6b-8bf0-b1c40ef941c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size: int, nb_classes: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size: the dimension of the input embeddings.\n",
    "        nb_classes: the output dimension.\n",
    "        \"\"\"\n",
    "        # your code\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: an input tensor\n",
    "        Returns:\n",
    "            Logits.\n",
    "        \"\"\"\n",
    "        # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482c601-8ad2-4244-aa49-380df76091c1",
   "metadata": {},
   "source": [
    "## Training (3 points)\n",
    "\n",
    "We put everything above together and train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a6da3-d538-4ebc-9bc9-cdec010de29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d80e77-75dc-4aa6-b0cf-0f4619bb8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train)\n",
    "valid_gen = lambda: data_generator(X_valid, y_valid)\n",
    "test_gen = lambda: data_generator(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddcbc0-a8a0-433f-86d6-63d51f1877c6",
   "metadata": {},
   "source": [
    "**\\[3 points\\] Fill the following cells. Make sure you save the best model evaluated on the validation set.**\n",
    "* The `deepcopy` function might help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea0bd2-d156-48e4-ab10-82dae2a880f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleClassifer(300, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# You can use another optimizer if you want.\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b4d32-9d63-4764-92e1-fa7ee27891f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 50\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "best_model = model\n",
    "best_validation_loss = np.Inf\n",
    "\n",
    "for epoch in tqdm(range(nb_epochs)):\n",
    "\n",
    "    # training\n",
    "    model.train()\n",
    "    # training loop\n",
    "    \n",
    "    # validation\n",
    "    model.eval()\n",
    "    # validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7907de12-1fc2-4597-980b-87d79baeb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(valid_losses, label=\"valid loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3020533b-0522-42ff-bcdc-6d1cd991d332",
   "metadata": {},
   "source": [
    "## Evaluation (3 point)\n",
    "\n",
    "**\\[1 point\\] Compute the accuracy for the 3 splits (training, validation, test).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73674e1e-0be6-49b0-af23-a0a9b518d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b7102-fb1d-4a27-a400-ff0e40e1a685",
   "metadata": {},
   "source": [
    "**\\[1 point\\] For two wrongly classified samples, try guessing why the model was wrong.**\n",
    "\n",
    "**\\[1 point\\] Code a `predict` function which take some text as input and returns a prediction class and score (the output of the sigmoid).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f7ac3-ad53-43e3-a1f6-90ed3a61259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    text: str,\n",
    "    text_pipeline: Callable[[str], torch.Tensor],\n",
    "    model: nn.Module,\n",
    "    device: str,\n",
    ") -> Tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Return the predicted class and score for a given input.\n",
    "    Args:\n",
    "        text: a given review.\n",
    "        text_pipeline: a function taking a text as input and returning a tensor (model's input).\n",
    "        model: a pre-trained model.\n",
    "        device: the device on which the computation occurs.\n",
    "    Returns:\n",
    "        A tuple (label, score).\n",
    "    \"\"\"\n",
    "    # Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7130f-ce7f-41ed-a7d2-bcc6f65482f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In my long years as a movie reviewers, I have seen good and bad movies. But nothing as controversially in the middle.\"\n",
    "predict(text, text_pipeline, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f2dff-da04-466e-bf2c-32f9e84d10a3",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Modify the classifier. Instead of using a simple logistic regression, create a multilayer perceptron. Something like `input -> linear(embedding_size, 128) -> activation function -> linear(128, nb_classes) -> output`, for a two layer perceptron.\n",
    "\n",
    "For the activation function, you can use [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) or [another non-linear activation function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) of your choice.\n",
    "\n",
    "Train your new classifier, look at the loss, and compare its accuracy with the logistic regression. Keep the model with the best validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8975c53d-2707-4d3c-ab8e-6962d5256136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
